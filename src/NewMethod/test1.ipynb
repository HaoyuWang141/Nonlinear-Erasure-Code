{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n",
      "original dir:  f:\\MyCourse(5 delayed 1)\\erasure code\\non-linear erasure code\\src\\NewMethod\n",
      "changed dir:  f:\\MyCourse(5 delayed 1)\\erasure code\\non-linear erasure code\\src\n"
     ]
    }
   ],
   "source": [
    "# You only need to run this section once.\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "\n",
    "print(\"original dir: \", os.getcwd())\n",
    "\n",
    "if os.getcwd().endswith(\"NewMethod\"):\n",
    "    new_path = \"../\"\n",
    "    os.chdir(new_path)\n",
    "    print(\"changed dir: \", os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "TASK_CONFIG = {\n",
    "    \"TASK\": \"CIFAR10\",\n",
    "    \"DATE\": datetime.datetime.now().strftime(\"%Y_%m_%d\"),\n",
    "    \"MODEL\": \"ResNet18\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读取数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 设置数据转换\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))]\n",
    ")\n",
    "\n",
    "# 设置数据集（训练集与测试集合）\n",
    "\n",
    "\"\"\"\n",
    "CIFAR-10:\n",
    "image: (3, 32, 32), label: (0-9)\n",
    "train_dataset: [(image, label), (image, label), ...], len(train_dataset): 50000\n",
    "test_dataset: [(image, label), (image, label), ...], len(test_dataset): 10000\n",
    "\"\"\"\n",
    "\n",
    "# CIFAR-10\n",
    "dataset_name = TASK_CONFIG[\"TASK\"]\n",
    "train_dataset = datasets.CIFAR10(\n",
    "    root=\"./data\", train=True, download=True, transform=transform\n",
    ")\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_dataset = datasets.CIFAR10(\n",
    "    root=\"./data\", train=False, download=True, transform=transform\n",
    ")\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "设置部分超参"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K: 4\n",
      "R: 0\n",
      "N: 4\n",
      "data_shape: (3, 32, 32)\n",
      "num_classes: 10\n",
      "base_model_path: ./base_model/ResNet18/CIFAR10/2024_02_06/model.pth\n",
      "encoder_path: ./encoder/MLP/CIFAR10/2024_02_16/encoder-task_CIFAR10-basemodel_ResNet18-in_4-out_0.pth\n",
      "decoder_path: ./decoder/MLP/CIFAR10/2024_02_16/decoder-task_CIFAR10-basemodel_ResNet18-in_4-out_4.pth\n",
      "save_dir: ./save/CIFAR10/ResNet18/2024_02_16/\n",
      "epoch_num: 4\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "K = 4\n",
    "R = 0\n",
    "N = K + R\n",
    "original_data_shape = tuple(train_dataset[0][0].shape)\n",
    "num_classes = 10\n",
    "print(f\"K: {K}\")\n",
    "print(f\"R: {R}\")\n",
    "print(f\"N: {N}\")\n",
    "print(f\"data_shape: {original_data_shape}\")\n",
    "print(f\"num_classes: {num_classes}\")\n",
    "\n",
    "base_model_path = f\"./base_model/{TASK_CONFIG['MODEL']}/{TASK_CONFIG['TASK']}/2024_02_06/model.pth\"\n",
    "encoder_path = f\"./encoder/MLP/{TASK_CONFIG['TASK']}/{TASK_CONFIG['DATE']}/encoder-task_{TASK_CONFIG['TASK']}-basemodel_{TASK_CONFIG['MODEL']}-in_{K}-out_{R}.pth\"\n",
    "decoder_path = f\"./decoder/MLP/{TASK_CONFIG['TASK']}/{TASK_CONFIG['DATE']}/decoder-task_{TASK_CONFIG['TASK']}-basemodel_{TASK_CONFIG['MODEL']}-in_{N}-out_{K}.pth\"\n",
    "save_dir = f\"./save/{TASK_CONFIG['TASK']}/{TASK_CONFIG['MODEL']}/{TASK_CONFIG['DATE']}/\"\n",
    "print(f\"base_model_path: {base_model_path}\")\n",
    "print(f\"encoder_path: {encoder_path}\")\n",
    "print(f\"decoder_path: {decoder_path}\")\n",
    "print(f\"save_dir: {save_dir}\")\n",
    "\n",
    "epoch_num = 4\n",
    "print(f\"epoch_num: {epoch_num}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义 base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-1.3150, -1.9806, -2.3473, -0.3044,  4.4703,  1.2104,  1.8691,  0.3243,\n",
      "         -5.7492,  2.7389]])\n",
      "tensor([[-1.3150, -1.9806, -2.3473, -0.3044,  4.4703,  1.2104,  1.8691,  0.3243,\n",
      "         -5.7492,  2.7389]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from base_model.VGG16 import VGG16\n",
    "from base_model.ResNet18 import ResNet18\n",
    "\n",
    "# 引入 base model, 该model将在后续全部过程中使用\n",
    "# ResNet\n",
    "assert TASK_CONFIG[\"MODEL\"] == \"ResNet18\"\n",
    "model = ResNet18(input_dim=original_data_shape, num_classes=num_classes)\n",
    "\n",
    "# 读取模型\n",
    "model.load_state_dict(torch.load(base_model_path))\n",
    "\n",
    "conv_segment = model.get_conv_segment()\n",
    "fc_segment = model.get_fc_segment()\n",
    "\n",
    "x = torch.randn(1, *original_data_shape)\n",
    "\n",
    "print(model(x).data)\n",
    "\n",
    "y = conv_segment(x)\n",
    "y = y.view(y.size(0), -1)\n",
    "y = fc_segment(y)\n",
    "print(y.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "设置另一部分超参"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv_output_data_shape: (512, 4, 4)\n",
      "split_conv_output_data_shape: (512, 4, 1)\n",
      "split_data_shape: (3, 32, 8)\n",
      "split_conv_output_data_shape from split_data_shape: (1, 512, 4, 1)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "conv_output_data_shape = model.calculate_conv_output(input_dim=original_data_shape)\n",
    "print(f\"conv_output_data_shape: {conv_output_data_shape}\")\n",
    "\n",
    "assert conv_output_data_shape[2] % K == 0\n",
    "split_conv_output_data_shape = tuple(\n",
    "    [conv_output_data_shape[0], conv_output_data_shape[1], conv_output_data_shape[2] // K]\n",
    ")\n",
    "print(f\"split_conv_output_data_shape: {split_conv_output_data_shape}\")\n",
    "\n",
    "split_data_shape = (3, 32, 8)\n",
    "print(f\"split_data_shape: {split_data_shape}\")\n",
    "# print(conv_segment)\n",
    "print(f\"split_conv_output_data_shape from split_data_shape: {tuple(conv_segment(torch.randn(1, *split_data_shape)).shape)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义 Encoder, Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from encoder.mlp_encoder import MLPEncoder\n",
    "from encoder.conv_encoder import CatChannelConvEncoder, CatBatchSizeConvEncoder\n",
    "from decoder.mlp_decoder import MLPDecoder\n",
    "from decoder.conv_decoder import CatChannelConvDecoder, CatBatchSizeConvDecoder\n",
    "\n",
    "# encoder = MLPEncoder(num_in=K, num_out=R, in_dim=split_data_shape)\n",
    "# decoder = MLPDecoder(num_in=N, num_out=K, in_dim=split_conv_output_data_shape)\n",
    "\n",
    "encoder = CatChannelConvEncoder(num_in=K, num_out=R, in_dim=split_data_shape)\n",
    "decoder = CatChannelConvDecoder(num_in=N, num_out=K, in_dim=split_conv_output_data_shape)\n",
    "\n",
    "# encoder = CatBatchSizeConvEncoder(num_in=K, num_out=R, in_dim=split_data_shape)\n",
    "# decoder = CatBatchSizeConvDecoder(num_in=N, num_out=K, in_dim=split_conv_output_data_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练 Encoder, Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset: 50000\n",
      "Test dataset: 10000\n",
      "image size:  torch.Size([3, 32, 32])\n",
      "Device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/4: 100%|████████████████████| 782/782 [04:06<00:00,  3.17it/s, loss=0.988]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1, Loss: 0.9884555339813232\n",
      "Train Accuracy: 10.0%\n",
      "Original Accuracy: 65.448%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/4:   5%|█                   | 43/782 [00:13<03:56,  3.13it/s, loss=0.953]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mf:\\MyCourse(5 delayed 1)\\erasure code\\non-linear erasure code\\src\\NewMethod\\test1.ipynb 单元格 14\u001b[0m line \u001b[0;36m6\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/MyCourse%285%20delayed%201%29/erasure%20code/non-linear%20erasure%20code/src/NewMethod/test1.ipynb#X20sZmlsZQ%3D%3D?line=59'>60</a>\u001b[0m labels \u001b[39m=\u001b[39m labels\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/MyCourse%285%20delayed%201%29/erasure%20code/non-linear%20erasure%20code/src/NewMethod/test1.ipynb#X20sZmlsZQ%3D%3D?line=61'>62</a>\u001b[0m images \u001b[39m=\u001b[39m images\u001b[39m.\u001b[39mto(device)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/f%3A/MyCourse%285%20delayed%201%29/erasure%20code/non-linear%20erasure%20code/src/NewMethod/test1.ipynb#X20sZmlsZQ%3D%3D?line=62'>63</a>\u001b[0m ground_truth \u001b[39m=\u001b[39m conv_segment(images)\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/MyCourse%285%20delayed%201%29/erasure%20code/non-linear%20erasure%20code/src/NewMethod/test1.ipynb#X20sZmlsZQ%3D%3D?line=63'>64</a>\u001b[0m ground_truth \u001b[39m=\u001b[39m ground_truth\u001b[39m.\u001b[39mview(ground_truth\u001b[39m.\u001b[39msize(\u001b[39m0\u001b[39m), \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     <a href='vscode-notebook-cell:/f%3A/MyCourse%285%20delayed%201%29/erasure%20code/non-linear%20erasure%20code/src/NewMethod/test1.ipynb#X20sZmlsZQ%3D%3D?line=65'>66</a>\u001b[0m \u001b[39m# forward\u001b[39;00m\n",
      "File \u001b[1;32mg:\\miniconda3\\envs\\ming\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mg:\\miniconda3\\envs\\ming\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mg:\\miniconda3\\envs\\ming\\Lib\\site-packages\\torch\\nn\\modules\\container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m    214\u001b[0m     \u001b[39mfor\u001b[39;00m module \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m:\n\u001b[1;32m--> 215\u001b[0m         \u001b[39minput\u001b[39m \u001b[39m=\u001b[39m module(\u001b[39minput\u001b[39;49m)\n\u001b[0;32m    216\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39minput\u001b[39m\n",
      "File \u001b[1;32mg:\\miniconda3\\envs\\ming\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mg:\\miniconda3\\envs\\ming\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mf:\\MyCourse(5 delayed 1)\\erasure code\\non-linear erasure code\\src\\base_model\\ResNet18.py:35\u001b[0m, in \u001b[0;36mBasicBlock.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     32\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrelu(out)\n\u001b[0;32m     34\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconv2(out)\n\u001b[1;32m---> 35\u001b[0m out \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbn2(out)\n\u001b[0;32m     37\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdownsample \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m     38\u001b[0m     residual \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdownsample(x)\n",
      "File \u001b[1;32mg:\\miniconda3\\envs\\ming\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[1;32mg:\\miniconda3\\envs\\ming\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[1;32mg:\\miniconda3\\envs\\ming\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:171\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    164\u001b[0m     bn_training \u001b[39m=\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_mean \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m) \u001b[39mand\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrunning_var \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m)\n\u001b[0;32m    166\u001b[0m \u001b[39m\u001b[39m\u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    167\u001b[0m \u001b[39mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[0;32m    168\u001b[0m \u001b[39mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[0;32m    169\u001b[0m \u001b[39mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[0;32m    170\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 171\u001b[0m \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mbatch_norm(\n\u001b[0;32m    172\u001b[0m     \u001b[39minput\u001b[39;49m,\n\u001b[0;32m    173\u001b[0m     \u001b[39m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[0;32m    174\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunning_mean\n\u001b[0;32m    175\u001b[0m     \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrack_running_stats\n\u001b[0;32m    176\u001b[0m     \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    177\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrunning_var \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining \u001b[39mor\u001b[39;49;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrack_running_stats \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m    178\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[0;32m    179\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias,\n\u001b[0;32m    180\u001b[0m     bn_training,\n\u001b[0;32m    181\u001b[0m     exponential_average_factor,\n\u001b[0;32m    182\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49meps,\n\u001b[0;32m    183\u001b[0m )\n",
      "File \u001b[1;32mg:\\miniconda3\\envs\\ming\\Lib\\site-packages\\torch\\nn\\functional.py:2478\u001b[0m, in \u001b[0;36mbatch_norm\u001b[1;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[0;32m   2475\u001b[0m \u001b[39mif\u001b[39;00m training:\n\u001b[0;32m   2476\u001b[0m     _verify_batch_size(\u001b[39minput\u001b[39m\u001b[39m.\u001b[39msize())\n\u001b[1;32m-> 2478\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49mbatch_norm(\n\u001b[0;32m   2479\u001b[0m     \u001b[39minput\u001b[39;49m, weight, bias, running_mean, running_var, training, momentum, eps, torch\u001b[39m.\u001b[39;49mbackends\u001b[39m.\u001b[39;49mcudnn\u001b[39m.\u001b[39;49menabled\n\u001b[0;32m   2480\u001b[0m )\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "from dataset.image_dataset import ImageDataset\n",
    "from util.split_data import split_vector\n",
    "\n",
    "print(f\"Train dataset: {len(train_dataset)}\")\n",
    "print(f\"Test dataset: {len(test_dataset)}\")\n",
    "print(\"image size: \", train_dataset[0][0].size())\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "# 定义损失函数\n",
    "criterion = nn.MSELoss()\n",
    "criterion2 = nn.CrossEntropyLoss()\n",
    "optimizer_encoder = optim.SGD(encoder.parameters(), lr=0.001, momentum=0.9)\n",
    "optimizer_decoder = optim.SGD(decoder.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "conv_segment.to(device)\n",
    "fc_segment.to(device)\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n",
    "\n",
    "conv_segment.eval()\n",
    "fc_segment.eval()\n",
    "encoder.train()\n",
    "decoder.train()\n",
    "\n",
    "model.eval()\n",
    "for m in model.modules():\n",
    "    if isinstance(m, nn.BatchNorm2d):\n",
    "        m.track_running_stats = False\n",
    "\n",
    "loss_list = [[] for _ in range(epoch_num)]\n",
    "\n",
    "for epoch in range(epoch_num):\n",
    "    train_loader_tqdm = tqdm(\n",
    "        train_loader,\n",
    "        desc=f\"Epoch {epoch+1}/{epoch_num}\",\n",
    "        bar_format=\"{l_bar}{bar:20}{r_bar}\",\n",
    "    )\n",
    "    correct = 0\n",
    "    correct_truth = 0\n",
    "    total = 0\n",
    "    for images, labels in train_loader_tqdm:\n",
    "        # split image tensor(64, 3, 32, 32) -> [tensor(64, 3, 32, 8) * K]\n",
    "        images_list = []\n",
    "        for start, end in split_vector(\n",
    "            L=original_data_shape[2], k=K, l=split_data_shape[2]\n",
    "        ):\n",
    "            images_list.append(images[:, :, :, start:end].clone())\n",
    "\n",
    "        images_list = [images.to(device) for images in images_list]\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        images = images.to(device)\n",
    "        ground_truth = conv_segment(images)\n",
    "        ground_truth = ground_truth.view(ground_truth.size(0), -1)\n",
    "\n",
    "        # forward\n",
    "        images_list += encoder(images_list)\n",
    "        output_list = []\n",
    "        for i in range(N):\n",
    "            output = conv_segment(images_list[i])\n",
    "            output_list.append(output)\n",
    "        # losed_output_list = lose_something(output_list, self.lose_device_index)\n",
    "        decoded_output_list = decoder(output_list)\n",
    "        output = torch.cat(decoded_output_list, dim=3)\n",
    "        output = output.view(output.size(0), -1)\n",
    "\n",
    "        loss = criterion(output, ground_truth)\n",
    "        # loss = criterion2(fc_segment(output), fc_segment(ground_truth))\n",
    "        loss_list[epoch].append(loss.item())\n",
    "\n",
    "        # backward\n",
    "        optimizer_encoder.zero_grad()\n",
    "        optimizer_decoder.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_encoder.step()\n",
    "        optimizer_decoder.step()\n",
    "\n",
    "        # calculate accuracy\n",
    "        _, predicted = torch.max(fc_segment(output).data, 1)\n",
    "        _, predicted_truth = torch.max(fc_segment(ground_truth.data), 1)\n",
    "        # print(predicted)\n",
    "        # print(predicted_truth)\n",
    "        # print(labels)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        correct_truth += (predicted_truth == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "        train_loader_tqdm.set_postfix(loss=loss.item())\n",
    "\n",
    "    print(f\"Epoch: {epoch+1}, Loss: {loss.item()}\")\n",
    "    print(f\"Train Accuracy: {100 * correct / total}%\")\n",
    "    print(f\"Original Accuracy: {100 * correct_truth / total}%\")\n",
    "    # 27%\n",
    "    # 10%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "保存模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "os.makedirs(os.path.dirname(encoder_path), exist_ok=True)\n",
    "os.makedirs(os.path.dirname(decoder_path), exist_ok=True)\n",
    "\n",
    "torch.save(encoder.state_dict(), encoder_path)\n",
    "torch.save(decoder.state_dict(), decoder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.2601187229156494, 0.26048094034194946, 0.26030564308166504, 0.2600433826446533, 0.26013463735580444, 0.25998711585998535, 0.2597982883453369, 0.2600659132003784, 0.2594950795173645, 0.259482741355896, 0.25943613052368164, 0.25985145568847656, 0.25924673676490784, 0.25947314500808716, 0.25912952423095703, 0.2587016224861145, 0.25878411531448364, 0.2591218054294586, 0.25886058807373047, 0.2581619620323181, 0.25840649008750916, 0.258434534072876, 0.25770869851112366, 0.2577586770057678, 0.25765109062194824, 0.2580875754356384, 0.25721490383148193, 0.25733238458633423, 0.2572248578071594, 0.25699079036712646, 0.2568736970424652, 0.2571099102497101, 0.2567161023616791, 0.2565695643424988, 0.256642609834671, 0.2560160756111145, 0.2559460997581482, 0.25592660903930664, 0.256087064743042, 0.2556203007698059, 0.25509336590766907, 0.25479450821876526, 0.25526127219200134, 0.2549249827861786, 0.2544858753681183, 0.25484490394592285, 0.2543456554412842, 0.2542656362056732, 0.25426825881004333, 0.2540573477745056, 0.2538030743598938, 0.2541658282279968, 0.253843754529953, 0.25337886810302734, 0.25308430194854736, 0.2532346248626709, 0.25297677516937256, 0.2529345452785492, 0.25269579887390137, 0.25228023529052734, 0.2524554133415222, 0.2524724006652832, 0.25199460983276367, 0.2523775100708008, 0.25229066610336304, 0.25209593772888184, 0.25178492069244385, 0.2518444061279297, 0.25145649909973145, 0.250962495803833, 0.25095608830451965, 0.25125595927238464, 0.25087547302246094, 0.25065577030181885, 0.2505751848220825, 0.25051623582839966, 0.25057902932167053, 0.2501254081726074, 0.2499237060546875, 0.2501565217971802, 0.2496240735054016, 0.24975475668907166, 0.2489892840385437, 0.24927961826324463, 0.24927684664726257, 0.2488987147808075, 0.24893344938755035, 0.24889016151428223, 0.24871481955051422, 0.2485995888710022, 0.24806982278823853, 0.24815738201141357, 0.24842655658721924, 0.24823036789894104, 0.24777787923812866, 0.24781103432178497, 0.24757695198059082, 0.24742558598518372, 0.2472420036792755, 0.24732108414173126, 0.247056782245636, 0.24708181619644165, 0.24656669795513153, 0.2462182492017746, 0.24628552794456482, 0.24633032083511353, 0.2460344135761261, 0.24589017033576965, 0.24567899107933044, 0.24577748775482178, 0.2454940378665924, 0.24568229913711548, 0.24571263790130615, 0.2452755868434906, 0.2451302856206894, 0.24505013227462769, 0.2445850372314453, 0.2447359263896942, 0.2445642650127411, 0.2440890520811081, 0.24447178840637207, 0.2442096322774887, 0.24400413036346436, 0.24343064427375793, 0.24349473416805267, 0.24370509386062622, 0.2436036467552185, 0.24341630935668945, 0.24289044737815857, 0.2430362105369568, 0.24305440485477448, 0.24292153120040894, 0.24257999658584595, 0.24234561622142792, 0.24236652255058289, 0.2423093020915985, 0.24201713502407074, 0.24189914762973785, 0.2416382133960724, 0.24172574281692505, 0.24139706790447235, 0.2413392812013626, 0.24089714884757996, 0.24095334112644196, 0.24080637097358704, 0.24005275964736938, 0.2404818832874298, 0.2406291365623474, 0.23995764553546906, 0.23966124653816223, 0.2398008406162262, 0.23963651061058044, 0.2395016849040985, 0.23950552940368652, 0.23924283683300018, 0.23938822746276855, 0.2389563024044037, 0.2390264868736267, 0.23863913118839264, 0.23856070637702942, 0.2382521629333496, 0.23806221783161163, 0.23747363686561584, 0.23794995248317719, 0.23743882775306702, 0.23741686344146729, 0.23764684796333313, 0.23671667277812958, 0.23681117594242096, 0.23697903752326965, 0.23742100596427917, 0.2364967316389084, 0.23648028075695038, 0.23624970018863678, 0.23614294826984406, 0.236322283744812, 0.23601661622524261, 0.23544926941394806, 0.23520366847515106, 0.2355615794658661, 0.23521840572357178, 0.23517148196697235, 0.2347785234451294, 0.23464804887771606, 0.2344711869955063, 0.23441700637340546, 0.23384340107440948, 0.23407769203186035, 0.23403936624526978, 0.23380115628242493, 0.23335807025432587, 0.23320429027080536, 0.23302815854549408, 0.23307597637176514, 0.2327013909816742, 0.2324220985174179, 0.23203551769256592, 0.23183900117874146, 0.23196551203727722, 0.23185651004314423, 0.23174777626991272, 0.2315596640110016, 0.23125821352005005, 0.23095911741256714, 0.23075029253959656, 0.23082441091537476, 0.23018001019954681, 0.2303737998008728, 0.23019489645957947, 0.23034977912902832, 0.22964024543762207, 0.22986005246639252, 0.2294582575559616, 0.22907906770706177, 0.22908160090446472, 0.22888922691345215, 0.22837188839912415, 0.2283661812543869, 0.22827275097370148, 0.2282218039035797, 0.22797760367393494, 0.22782254219055176, 0.2269335687160492, 0.22735276818275452, 0.22700589895248413, 0.22706037759780884, 0.22692272067070007, 0.22645117342472076, 0.2261049449443817, 0.225763738155365, 0.22571131587028503, 0.22529979050159454, 0.22552266716957092, 0.22504685819149017, 0.22512677311897278, 0.22445285320281982, 0.22454652190208435, 0.22425006330013275, 0.2240343689918518, 0.22353500127792358, 0.22354479134082794, 0.2235531061887741, 0.22323699295520782, 0.22327293455600739, 0.2228357046842575, 0.22241266071796417, 0.22211313247680664, 0.22202785313129425, 0.2216435968875885, 0.22125780582427979, 0.2214476466178894, 0.2213664948940277, 0.22064176201820374, 0.22070163488388062, 0.22039853036403656, 0.2204395830631256, 0.21988342702388763, 0.21963763236999512, 0.21948197484016418, 0.219168022274971, 0.21919037401676178, 0.2187243402004242, 0.21860098838806152, 0.21822784841060638, 0.2180233895778656, 0.21788662672042847, 0.21749314665794373, 0.2171764075756073, 0.2171810120344162, 0.21642175316810608, 0.2167288362979889, 0.2164306640625, 0.21582484245300293, 0.21583613753318787, 0.21574023365974426, 0.21497787535190582, 0.21503010392189026, 0.2146642506122589, 0.21407435834407806, 0.2140108346939087, 0.2138211578130722, 0.2137310951948166, 0.2133042812347412, 0.21312452852725983, 0.21263501048088074, 0.2127053588628769, 0.2121204137802124, 0.21217937767505646, 0.2118656039237976, 0.2114773690700531, 0.2108335644006729, 0.21066172420978546, 0.2106497585773468, 0.2100050151348114, 0.2098904401063919, 0.2097020149230957, 0.2094993144273758, 0.20913976430892944, 0.20858179032802582, 0.2083401083946228, 0.20829521119594574, 0.20815470814704895, 0.20773299038410187, 0.2072443962097168, 0.20689493417739868, 0.2065276801586151, 0.2063293755054474, 0.20583359897136688, 0.2056410163640976, 0.20514050126075745, 0.2052871733903885, 0.20495638251304626, 0.20447593927383423, 0.20382159948349, 0.20396503806114197, 0.20331645011901855, 0.20303604006767273, 0.20289009809494019, 0.2024226188659668, 0.20190680027008057, 0.20167985558509827, 0.20132455229759216, 0.20108002424240112, 0.20120832324028015, 0.200340136885643, 0.2003108561038971, 0.1997111737728119, 0.19923293590545654, 0.19914588332176208, 0.19878926873207092, 0.19846795499324799, 0.19780504703521729, 0.19766928255558014, 0.19727694988250732, 0.19694927334785461, 0.1965927928686142, 0.19616740942001343, 0.19563394784927368, 0.1958564817905426, 0.19502711296081543, 0.1950134038925171, 0.19434623420238495, 0.1938275694847107, 0.19360670447349548, 0.19331036508083344, 0.19286850094795227, 0.19262553751468658, 0.19217616319656372, 0.1919567584991455, 0.19135719537734985, 0.19105902314186096, 0.19054767489433289, 0.1898355782032013, 0.18969736993312836, 0.1891772300004959, 0.18891313672065735, 0.18841968476772308, 0.18795326352119446, 0.18800905346870422, 0.18713444471359253, 0.18684276938438416, 0.18643659353256226, 0.18588152527809143, 0.18592776358127594, 0.18487367033958435, 0.18491289019584656, 0.18414878845214844, 0.18372909724712372, 0.18379655480384827, 0.18322524428367615, 0.18248893320560455, 0.1822194755077362, 0.18180355429649353, 0.18144617974758148, 0.18109403550624847, 0.1802235245704651, 0.18008944392204285, 0.1793247014284134, 0.17885097861289978, 0.1786128580570221, 0.17829501628875732, 0.17762213945388794, 0.17717868089675903, 0.17696544528007507, 0.1766262650489807, 0.17590714991092682, 0.1755182296037674, 0.1750984936952591, 0.1745395064353943, 0.17372825741767883, 0.1739462912082672, 0.17294177412986755, 0.17308475077152252, 0.17214134335517883, 0.17188069224357605, 0.17134055495262146, 0.17059537768363953, 0.1701730340719223, 0.16963957250118256, 0.16931745409965515, 0.1687316745519638, 0.16841909289360046, 0.16791704297065735, 0.16739320755004883, 0.16716140508651733, 0.16610486805438995, 0.16559627652168274, 0.16511571407318115, 0.16460895538330078, 0.16426852345466614, 0.16367706656455994, 0.16332665085792542, 0.16250810027122498, 0.16188062727451324, 0.16169002652168274, 0.16120563447475433, 0.1608380675315857, 0.15988004207611084, 0.15949417650699615, 0.15924815833568573, 0.15828832983970642, 0.15777547657489777, 0.15737488865852356, 0.15662872791290283, 0.15618976950645447, 0.15557710826396942, 0.1551719456911087, 0.15457290410995483, 0.15403345227241516, 0.1536904275417328, 0.15289953351020813, 0.15235385298728943, 0.15190446376800537, 0.15140469372272491, 0.15097488462924957, 0.15014810860157013, 0.14966930449008942, 0.14898569881916046, 0.1485760509967804, 0.1479920595884323, 0.14761032164096832, 0.14656446874141693, 0.1461487114429474, 0.14589554071426392, 0.1450282633304596, 0.14430741965770721, 0.14388307929039001, 0.14324849843978882, 0.14281496405601501, 0.14190535247325897, 0.14181570708751678, 0.1410733312368393, 0.14059734344482422, 0.13980063796043396, 0.139293372631073, 0.13848735392093658, 0.13807523250579834, 0.1374201625585556, 0.1369493156671524, 0.13612550497055054, 0.13564568758010864, 0.13486695289611816, 0.13426640629768372, 0.13367877900600433, 0.1331721544265747, 0.1325961947441101, 0.1321408897638321, 0.1314813494682312, 0.13043364882469177, 0.13006657361984253, 0.12955161929130554, 0.12869176268577576, 0.1281965672969818, 0.12749610841274261, 0.12700816988945007, 0.12620238959789276, 0.12593024969100952, 0.12513646483421326, 0.12480145692825317, 0.12348837405443192, 0.12322287261486053, 0.12267452478408813, 0.12199661135673523, 0.12128719687461853, 0.1205296590924263, 0.12004397064447403, 0.11918490380048752, 0.1188129186630249, 0.11808153986930847, 0.11759048700332642, 0.11691449582576752, 0.11598671227693558, 0.11556369811296463, 0.11516355723142624, 0.11432822793722153, 0.11353220045566559, 0.11300264298915863, 0.11255133897066116, 0.11171933263540268, 0.11112132668495178, 0.11023853719234467, 0.10995118319988251, 0.10920075327157974, 0.10865013301372528, 0.10815022885799408, 0.10730388015508652, 0.10670509934425354, 0.10596121847629547, 0.10513205826282501, 0.10467152297496796, 0.10411424189805984, 0.10347599536180496, 0.10283668339252472, 0.10206389427185059, 0.10167985409498215, 0.10089703649282455, 0.10031960904598236, 0.09965195506811142, 0.09904579073190689, 0.09836948662996292, 0.09781642258167267, 0.09726317226886749, 0.09639320522546768, 0.09591004252433777, 0.09510555863380432, 0.09450189024209976, 0.09409325569868088, 0.09320001304149628, 0.09256578981876373, 0.09223081171512604, 0.09123386442661285, 0.09075053036212921, 0.09019194543361664, 0.0895557850599289, 0.08893744647502899, 0.08853304386138916, 0.08775420486927032, 0.08734463155269623, 0.08656776696443558, 0.0859498679637909, 0.08515867590904236, 0.08480560779571533, 0.08393512666225433, 0.0835941731929779, 0.08288875222206116, 0.08247838914394379, 0.08173666894435883, 0.08108756691217422, 0.08049124479293823, 0.07972414791584015, 0.07920683920383453, 0.07845350354909897, 0.07798711955547333, 0.07750189304351807, 0.07694351673126221, 0.07620009034872055, 0.07570239901542664, 0.07535625994205475, 0.07450680434703827, 0.073967345058918, 0.07368706166744232, 0.07289645820856094, 0.07232591509819031, 0.07180939614772797, 0.07109246402978897, 0.07059232890605927, 0.07007694244384766, 0.06933681666851044, 0.06905803829431534, 0.06850679218769073, 0.06767548620700836, 0.06731785088777542, 0.0667259693145752, 0.06629838049411774, 0.06555555015802383, 0.06508801877498627, 0.0644989162683487, 0.0639830231666565, 0.0633925348520279, 0.06294417381286621, 0.06233479455113411, 0.061907194554805756, 0.061386168003082275, 0.060929879546165466, 0.060687728226184845, 0.05979049950838089, 0.059404876083135605, 0.058862630277872086, 0.058487553149461746, 0.05782444775104523, 0.057330891489982605, 0.05713145434856415, 0.05647087097167969, 0.05596335232257843, 0.055253203958272934, 0.05488616228103638, 0.054513365030288696, 0.053956396877765656, 0.05349031463265419, 0.05315253883600235, 0.05258350074291229, 0.05216857045888901, 0.05171896517276764, 0.05114462226629257, 0.05059824138879776, 0.05044294148683548, 0.04972867667675018, 0.04937126487493515, 0.048933327198028564, 0.048715271055698395, 0.04814954102039337, 0.04763564467430115, 0.047251299023628235, 0.04678495228290558, 0.046465400606393814, 0.04600583016872406, 0.045644305646419525, 0.04522323235869408, 0.04480981081724167, 0.044567130506038666, 0.043987810611724854, 0.04348115250468254, 0.04329854995012283, 0.042844027280807495, 0.042306192219257355, 0.04186895117163658, 0.04162626713514328, 0.04125739634037018, 0.040875352919101715, 0.040445372462272644, 0.04006769508123398, 0.03973757475614548, 0.03940265625715256, 0.039046723395586014, 0.038674790412187576, 0.0384250283241272, 0.03791765868663788, 0.03764108195900917, 0.03741496428847313, 0.0368952676653862, 0.03647272661328316, 0.03624444082379341, 0.035897497087717056, 0.0355251170694828, 0.0353643037378788, 0.0348183773458004, 0.034653257578611374, 0.034261688590049744, 0.03390512987971306, 0.033704787492752075, 0.033415768295526505, 0.033023059368133545, 0.03268655389547348, 0.03254837170243263, 0.032204970717430115, 0.031715549528598785, 0.03164035826921463, 0.03145531937479973, 0.031071636825799942, 0.030664289370179176, 0.030537420883774757, 0.030187204480171204, 0.029891207814216614, 0.029674198478460312, 0.029334845021367073, 0.02882574498653412, 0.028508396819233894, 0.02868628315627575, 0.028189705684781075, 0.02816532552242279, 0.027775976806879044, 0.027436567470431328, 0.0274062342941761, 0.027333848178386688, 0.026766642928123474, 0.026677723973989487, 0.02651762031018734, 0.02608497440814972, 0.02605520933866501, 0.02562043070793152, 0.025540554895997047, 0.02498406171798706, 0.02476966381072998, 0.02487540990114212, 0.02437329851090908, 0.02435772493481636, 0.024101320654153824, 0.023770563304424286, 0.02351139858365059, 0.023412950336933136, 0.02335645630955696, 0.022802453488111496, 0.022708535194396973, 0.022584939375519753, 0.022514663636684418, 0.022112296894192696, 0.022060511633753777, 0.021820219233632088, 0.021597465500235558, 0.021444179117679596, 0.02128683403134346, 0.021106749773025513, 0.020705977454781532, 0.020686687901616096, 0.020318271592259407, 0.020388558506965637, 0.020218893885612488, 0.01990165188908577, 0.01965022087097168, 0.0197942815721035, 0.019590718671679497, 0.019266881048679352, 0.019141551107168198, 0.01883070543408394, 0.018780307844281197, 0.018804945051670074, 0.01856406033039093, 0.018288983032107353, 0.018237750977277756, 0.01829187199473381, 0.017901739105582237, 0.01771518588066101, 0.017657412216067314, 0.017521850764751434, 0.017264727503061295, 0.0170297734439373, 0.017116332426667213, 0.016871951520442963, 0.016749776899814606, 0.01690608263015747, 0.016595181077718735, 0.016283998265862465, 0.016388261690735817, 0.01602950133383274, 0.01612839102745056, 0.015919247642159462, 0.015712354332208633, 0.015677589923143387, 0.015478762798011303, 0.015517208725214005, 0.015119172632694244, 0.015147616155445576, 0.014911728911101818, 0.014869090169668198, 0.014901328831911087, 0.01449604332447052, 0.014534673653542995, 0.014410188421607018, 0.014238005504012108, 0.014297676272690296, 0.014046001248061657, 0.013848775997757912, 0.01370617002248764, 0.01374177634716034, 0.013596178963780403, 0.013569609262049198, 0.013362806290388107, 0.013454657979309559, 0.013287059962749481, 0.013257781974971294, 0.012943661771714687, 0.013250667601823807, 0.012936800718307495, 0.01278613694012165, 0.012545708566904068, 0.01249123178422451, 0.012507690116763115, 0.012252457439899445, 0.012114102020859718, 0.01236848346889019, 0.01216195523738861, 0.011966517195105553, 0.011953426524996758, 0.011749140918254852, 0.011967597529292107, 0.01168094016611576, 0.011727822944521904, 0.011650370433926582, 0.011312898248434067, 0.011517470702528954, 0.011419208720326424, 0.01084943488240242] [0.011020415462553501, 0.011096805334091187, 0.010966747999191284, 0.01101729180663824, 0.01084873452782631, 0.010754314251244068, 0.010919978842139244, 0.010781679302453995, 0.010432533919811249, 0.010793907567858696, 0.010662819258868694, 0.010207148268818855, 0.010115315206348896, 0.01028088852763176, 0.009978583082556725, 0.010152315720915794, 0.010000145994126797, 0.010030237957835197, 0.009738344699144363, 0.00977952778339386, 0.009646683000028133, 0.009543795138597488, 0.009615009650588036, 0.009535691700875759, 0.009389352984726429, 0.009428298100829124, 0.009091981686651707, 0.009266179986298084, 0.009459147229790688, 0.009078733623027802, 0.008999403566122055, 0.009011298418045044, 0.00887015275657177, 0.008971285074949265, 0.009025655686855316, 0.008819760754704475, 0.008837186731398106, 0.008618565276265144, 0.008745148777961731, 0.008590552024543285, 0.008681101724505424, 0.008379238657653332, 0.008521043695509434, 0.008282739669084549, 0.008406504988670349, 0.008255726657807827, 0.008171478286385536, 0.00830266997218132, 0.008091037161648273, 0.008024079725146294, 0.008122242987155914, 0.007923009805381298, 0.008094431832432747, 0.007909954525530338, 0.007870800793170929, 0.007901223376393318, 0.007737535517662764, 0.007750965189188719, 0.007771523203700781, 0.0076185655780136585, 0.007700441405177116, 0.007689003832638264, 0.007402903866022825, 0.007654392160475254, 0.007579573430120945, 0.007238517515361309, 0.007371446117758751, 0.007490983232855797, 0.0072998302057385445, 0.007312675938010216, 0.007297230418771505, 0.007079571485519409, 0.007302036974579096, 0.007057623937726021, 0.007022527977824211, 0.007043324410915375, 0.006956537254154682, 0.00711436104029417, 0.007025962229818106, 0.007039619609713554, 0.006684586871415377, 0.006663274951279163, 0.006686322391033173, 0.006699767895042896, 0.0067629581317305565, 0.006598799489438534, 0.006727975327521563, 0.006855051964521408, 0.006657670252025127, 0.00669130589812994, 0.006583194714039564, 0.006292661651968956, 0.00642926711589098, 0.006109832786023617, 0.006616018712520599, 0.006351640447974205, 0.006308884359896183, 0.006367454305291176, 0.00661962665617466, 0.006405347026884556, 0.006287367083132267, 0.006322563625872135, 0.006158234551548958, 0.00613483227789402, 0.0061865076422691345, 0.00625994149595499, 0.006500077433884144, 0.0061654625460505486, 0.006001744419336319, 0.006256508640944958, 0.006005743518471718, 0.0059099216014146805, 0.005964374635368586, 0.005915124900639057, 0.005909920670092106, 0.005732248537242413, 0.005816386081278324, 0.005709328223019838, 0.005947613622993231, 0.005847151391208172, 0.005687188357114792, 0.005707389209419489, 0.005734832026064396, 0.005785787012428045, 0.0058870501816272736, 0.005596559960395098, 0.00565724354237318, 0.005700003355741501, 0.005783682689070702, 0.0055975597351789474, 0.005633624270558357, 0.005441711749881506, 0.0054484824649989605, 0.005576170980930328, 0.005403061397373676, 0.005293519701808691, 0.005499407649040222, 0.00524419080466032, 0.0055968607775866985, 0.0055824164301157, 0.005341855343431234, 0.00540345860645175, 0.005338055081665516, 0.005192087031900883, 0.005102232564240694, 0.005179991014301777, 0.005449201911687851, 0.005192121956497431, 0.005316995084285736, 0.005270447116345167, 0.005068783648312092, 0.0051781125366687775, 0.005211993120610714, 0.005094650201499462, 0.004977070726454258, 0.005155939608812332, 0.005174433812499046, 0.005224929191172123, 0.005122024100273848, 0.00500880740582943, 0.005549497436732054, 0.004978545941412449, 0.00486992672085762, 0.005004454404115677, 0.005022065714001656, 0.00493275374174118, 0.005056262016296387, 0.00528303999453783, 0.004906229209154844, 0.00483167776837945, 0.004946290049701929, 0.004928697366267443, 0.004809183534234762, 0.004915691912174225, 0.005285538733005524, 0.00478382408618927, 0.004691395442932844, 0.00480234669521451, 0.004723841790109873, 0.004567143972963095, 0.004582643508911133, 0.004775929264724255, 0.004872565623372793, 0.004525784403085709, 0.004666220396757126, 0.00485659297555685, 0.0044765775091946125, 0.004571728408336639, 0.0046036564745008945, 0.004630193114280701, 0.004725808277726173, 0.0045737032778561115, 0.004659630358219147, 0.004716104827821255, 0.004642056301236153, 0.004881083033978939, 0.004462087992578745, 0.00446576252579689, 0.0044877054169774055, 0.004468020983040333, 0.004773302935063839, 0.004515062086284161, 0.004759685136377811, 0.004635820165276527, 0.0043437182903289795, 0.004475641064345837, 0.004538008477538824, 0.0045729815028607845, 0.004338006488978863, 0.0043656909838318825, 0.004435374401509762, 0.00459099467843771, 0.0045068650506436825, 0.004387721884995699, 0.004255436360836029, 0.004521038383245468, 0.004413933493196964, 0.0047213067300617695, 0.004311033990234137, 0.004327785689383745, 0.004384725354611874, 0.00421198271214962, 0.004298236221075058, 0.004471485037356615, 0.004242781084030867, 0.004234436899423599, 0.004569942131638527, 0.004255907144397497, 0.004122432321310043, 0.004161378368735313, 0.004301527515053749, 0.004116331692785025, 0.004312313627451658, 0.004145916551351547, 0.0042971037328243256, 0.004140117671340704, 0.0044449749402701855, 0.004185542464256287, 0.004231099970638752, 0.004213741980493069, 0.003919187001883984, 0.0040925173088908195, 0.004390556365251541, 0.003886781632900238, 0.004385574720799923, 0.004117230419069529, 0.003995935432612896, 0.0040380279533565044, 0.004302641376852989, 0.004256514832377434, 0.004043664317578077, 0.003907423932105303, 0.004139624536037445, 0.004223939031362534, 0.0038213797379285097, 0.003972853533923626, 0.004284839145839214, 0.003925786819308996, 0.003941189032047987, 0.004203914664685726, 0.0038809224497526884, 0.003963548224419355, 0.004163314588367939, 0.0040504420176148415, 0.00397438695654273, 0.004027139861136675, 0.004139740020036697, 0.0039933049120008945, 0.004014203790575266, 0.004115773364901543, 0.003958248533308506, 0.003954879008233547, 0.003999844193458557, 0.004075502045452595, 0.0038988422602415085, 0.004033356439322233, 0.0040227691642940044, 0.004060045816004276, 0.004090757574886084, 0.0038785424549132586, 0.0038945325650274754, 0.0039682756178081036, 0.003869289066642523, 0.003985272720456123, 0.003969764336943626, 0.004030514974147081, 0.0038888573180884123, 0.003928870894014835, 0.004108617082238197, 0.003831451525911689, 0.004162551369518042, 0.004049040377140045, 0.0037693451158702374, 0.00377678289078176, 0.00381476734764874, 0.00395264383405447, 0.003929414786398411, 0.003855657996609807, 0.0038135715294629335, 0.0040243398398160934, 0.004070003051310778, 0.004172450862824917, 0.003940340131521225, 0.0037541224155575037, 0.003804902546107769, 0.0038503934629261494, 0.0037657874636352062, 0.0037779989652335644, 0.003770657815039158, 0.0036215230356901884, 0.003979823552072048, 0.0039438167586922646, 0.003822650294750929, 0.004015391692519188, 0.003915604203939438, 0.004014602862298489, 0.0037768862675875425, 0.0038707852363586426, 0.003827563254162669, 0.0036480044946074486, 0.0037734336219727993, 0.0036591372918337584, 0.0037415733095258474, 0.004015068989247084, 0.003600489813834429, 0.003822470549494028, 0.003802095539867878, 0.003805221291258931, 0.0038778509479016066, 0.004070465452969074, 0.0039499131962656975, 0.003551000962033868, 0.00373186101205647, 0.003668110352009535, 0.0037456403952091932, 0.003713990794494748, 0.0035871004220098257, 0.003790700575336814, 0.0036163695622235537, 0.003967409487813711, 0.0036956826224923134, 0.0038229040801525116, 0.00365222105756402, 0.003936110530048609, 0.004058884456753731, 0.003560209646821022, 0.003726284485310316, 0.0036774002946913242, 0.0035811220295727253, 0.0035788402892649174, 0.0036560201551765203, 0.0035830908454954624, 0.003643262665718794, 0.0034718357492238283, 0.0035064879339188337, 0.0035133203491568565, 0.003760908031836152, 0.0035698069259524345, 0.003709775162860751, 0.003600075375288725, 0.003885556012392044, 0.003563989419490099, 0.003712998703122139, 0.003826513886451721, 0.003677853848785162, 0.0036261887289583683, 0.0035638799890875816, 0.0037021583411842585, 0.003654568223282695, 0.003623881144449115, 0.0036135870032012463, 0.003560383338481188, 0.0036429527681320906, 0.0035140784457325935, 0.0035840203054249287, 0.0036058775149285793, 0.0037875089328736067, 0.003729855176061392, 0.0038326624780893326, 0.00356644531711936, 0.003795316908508539, 0.0037823012098670006, 0.003877429524436593, 0.0035201830323785543, 0.003479620208963752, 0.003534714225679636, 0.0037981076166033745, 0.0035783571656793356, 0.003780762664973736, 0.0034998867195099592, 0.0034876668360084295, 0.003418747102841735, 0.003549260552972555, 0.0037099304609000683, 0.00372873293235898, 0.0035784838255494833, 0.0035575341898947954, 0.0038414904847741127, 0.003920306451618671, 0.0036108267959207296, 0.003842429257929325, 0.003596023889258504, 0.0035722721368074417, 0.003540852339938283, 0.0035697161220014095, 0.003467625705525279, 0.003979211673140526, 0.0035336725413799286, 0.003485853783786297, 0.0035126651637256145, 0.0035598755348473787, 0.0034814034588634968, 0.003508682129904628, 0.0034752176143229008, 0.0035732171963900328, 0.0035367864184081554, 0.0036193898413330317, 0.003613147186115384, 0.0034766197204589844, 0.0036887251771986485, 0.003439279505982995, 0.0038132835179567337, 0.003559000790119171, 0.003413221798837185, 0.003725966438651085, 0.0037432555109262466, 0.0037242774851620197, 0.003579525277018547, 0.003474282566457987, 0.003877293784171343, 0.003640958108007908, 0.0035711415112018585, 0.0036841712426394224, 0.0037073041312396526, 0.0034073800779879093, 0.0037808124907314777, 0.0036932900547981262, 0.0037566956598311663, 0.0036390796303749084, 0.0034842700697481632, 0.0034642466343939304, 0.0035873372107744217, 0.003393301973119378, 0.003678128821775317, 0.0034710781183093786, 0.00355197093449533, 0.003289654850959778, 0.0037579357158392668, 0.003528371686115861, 0.0035240203142166138, 0.003663348965346813, 0.003429389325901866, 0.0033283052034676075, 0.003490894567221403, 0.003478161757811904, 0.0038282054010778666, 0.0035117466468364, 0.0034807641059160233, 0.0034126583486795425, 0.003446792019531131, 0.0035040173679590225, 0.0037369029596447945, 0.00356447696685791, 0.0033255808521062136, 0.003567778505384922, 0.0035100411623716354, 0.003293863497674465, 0.0035595460794866085, 0.0035384679213166237, 0.003584435908123851, 0.0035607367753982544, 0.0035807134117931128, 0.0036252171266824007, 0.00346496794372797, 0.003503873012959957, 0.003614146029576659, 0.003450163407251239, 0.0035670448560267687, 0.00352778984233737, 0.0033837559167295694, 0.0034426760394126177, 0.003492872230708599, 0.003555621486157179, 0.0035155080258846283, 0.003598391078412533, 0.0034901555627584457, 0.0034328140318393707, 0.003758546896278858, 0.0036848094314336777, 0.003717743791639805, 0.003470728872343898, 0.003379976376891136, 0.0033999080769717693, 0.003678138367831707, 0.003375684842467308, 0.003409669268876314, 0.003684327472001314, 0.0036161032039672136, 0.0036207467783242464, 0.003541185287758708, 0.00333360955119133, 0.0036846878938376904, 0.003449040465056896, 0.0034933362621814013, 0.003606900107115507, 0.003865963313728571, 0.0035924455150961876, 0.003525854554027319, 0.0035208892077207565, 0.003801441052928567, 0.0033440287224948406, 0.0036962477024644613, 0.00333849573507905, 0.003508806461468339, 0.0036312006413936615, 0.0034503673668950796, 0.0032970872707664967, 0.003597464645281434, 0.0035992292687296867, 0.003479239996522665, 0.003499063663184643, 0.0033466070890426636, 0.0034730443730950356, 0.0035371538251638412, 0.00344368489459157, 0.0037430631928145885, 0.003565774066373706, 0.0034139109775424004, 0.0036471302155405283, 0.003645555581897497, 0.0033427246380597353, 0.0035232778172940016, 0.003848266089335084, 0.0033284001983702183, 0.003366681281477213, 0.003537707030773163, 0.00339608546346426, 0.003550566267222166, 0.003455032128840685, 0.0036112507805228233, 0.003457714105024934, 0.003673349041491747, 0.0037576283793896437, 0.0033902269788086414, 0.0036608721129596233, 0.003535306081175804, 0.0034401044249534607, 0.003599298419430852, 0.0036299244966357946, 0.003589168656617403, 0.0035562077537178993, 0.003466517897322774, 0.0035313928965479136, 0.003573822556063533, 0.003349744249135256, 0.00362766208127141, 0.003471300471574068, 0.00350646092556417, 0.0034861271269619465, 0.0034802337177097797, 0.0035395785234868526, 0.0034475699067115784, 0.0035305959172546864, 0.003615842666476965, 0.0034740592818707228, 0.0034830989316105843, 0.0033657276071608067, 0.003607479389756918, 0.0034585599787533283, 0.0035040434449911118, 0.0033016016241163015, 0.0035686527844518423, 0.0037047783844172955, 0.0038563695270568132, 0.0034500653855502605, 0.003335717599838972, 0.0036938723642379045, 0.0034326950553804636, 0.0035965635906904936, 0.0034976215101778507, 0.0035240440629422665, 0.0034302864223718643, 0.0033809810411185026, 0.003476942889392376, 0.003412762889638543, 0.0033816122449934483, 0.0034471810795366764, 0.0036097208503633738, 0.0034209981095045805, 0.003284218953922391, 0.003396223299205303, 0.003611209336668253, 0.003757508471608162, 0.0034704250283539295, 0.003415066050365567, 0.003435291815549135, 0.003512556431815028, 0.0034528542309999466, 0.003345702774822712, 0.003564293961971998, 0.0035607810132205486, 0.0034689782187342644, 0.003387901932001114, 0.003437212435528636, 0.0034628473222255707, 0.0034968098625540733, 0.0033976053819060326, 0.003569856286048889, 0.0035095778293907642, 0.0035030446015298367, 0.0034610494039952755, 0.0033937604166567326, 0.0034467335790395737, 0.0034858044236898422, 0.0034167803823947906, 0.0033612975385040045, 0.003579109674319625, 0.00341281620785594, 0.0035242929589003325, 0.0036500210408121347, 0.0033443442080169916, 0.0034323378931730986, 0.0033370705787092447, 0.003567884676158428, 0.0034646408166736364, 0.003221777966246009, 0.00345858046784997, 0.0035927314311265945, 0.003638477763161063, 0.0032936641946434975, 0.0033632908016443253, 0.0038273967802524567, 0.003669571131467819, 0.003621515817940235, 0.003452068194746971, 0.0035094101913273335, 0.0036456461530178785, 0.003510258160531521, 0.003572626505047083, 0.003588607534766197, 0.0033922670409083366, 0.0035174433141946793, 0.0033564227633178234, 0.0036359357181936502, 0.003427634248510003, 0.003536168485879898, 0.003415325190871954, 0.0035736444406211376, 0.003387695411220193, 0.003482266329228878, 0.0033553168177604675, 0.003563418984413147, 0.003578819101676345, 0.003459346480667591, 0.003569704480469227, 0.0035266708582639694, 0.0035443876404315233, 0.0035218577831983566, 0.0035643507726490498, 0.003324683289974928, 0.0033563850447535515, 0.003679506480693817, 0.0033167381770908833, 0.0035246401093900204, 0.0033910858910530806, 0.003439599648118019, 0.003469758201390505, 0.0035046562552452087, 0.003589156549423933, 0.0034980226773768663, 0.003596239723265171, 0.003473989199846983, 0.00359205505810678, 0.0037502937484532595, 0.003506949171423912, 0.0035019072238355875, 0.0034036191646009684, 0.0035351465921849012, 0.0034464942291378975, 0.003519939724355936, 0.003449384355917573, 0.003392427461221814, 0.0035910652950406075, 0.003636544104665518, 0.003722617868334055, 0.003455917350947857, 0.003745816648006439, 0.003464085515588522, 0.003524783067405224, 0.003636888926848769, 0.0037617767229676247, 0.0035503688268363476, 0.003659231588244438, 0.0036424167919903994, 0.0033620293252170086, 0.0035555576905608177, 0.0034570712596178055, 0.0035104001872241497, 0.003666035830974579, 0.003601518925279379, 0.0034555126912891865, 0.0035505411215126514, 0.0036435704678297043, 0.003656485117971897, 0.00348049309104681, 0.003393335733562708, 0.00359153188765049, 0.003464195178821683, 0.003729782300069928, 0.0031910925172269344, 0.003370886668562889, 0.003428618423640728, 0.0034840800799429417, 0.003346964018419385, 0.0033753663301467896, 0.003387857461348176, 0.0033980393782258034, 0.003409062512218952, 0.003401931608095765, 0.003490733914077282, 0.003494527656584978, 0.003518100827932358, 0.0036116857081651688, 0.0036070251371711493, 0.00353100523352623, 0.0035018727649003267, 0.003562248544767499, 0.003428468946367502, 0.00359725346788764, 0.0033787463326007128, 0.0035815476439893246, 0.0032917142380028963, 0.003609847044572234, 0.0035443834494799376, 0.003370905527845025, 0.0034626491833478212, 0.003586491337046027, 0.003687313525006175, 0.0034266056027263403, 0.0033708419650793076, 0.0035777208395302296, 0.003490439150482416, 0.003447852563112974, 0.003492402844130993, 0.0033819866366684437, 0.003402430098503828, 0.0034028016962110996, 0.003385433228686452, 0.0036191940307617188, 0.0033582900650799274, 0.00339858909137547, 0.0034052757546305656, 0.0034729603212326765, 0.0034276964142918587, 0.003728727111592889, 0.0033650260884314775, 0.003375439904630184, 0.0037045113276690245, 0.003556923009455204, 0.00343380612321198, 0.003339131362736225, 0.003625244367867708, 0.003545502433553338, 0.0036664060316979885, 0.0034396275877952576, 0.003514286130666733, 0.003262124489992857, 0.0034547653049230576, 0.0033607217483222485, 0.003191309282556176, 0.003349107224494219, 0.0035762283951044083, 0.0032214680686593056, 0.0032490266021341085, 0.00375355570577085, 0.003427884541451931, 0.003417972009629011, 0.003341791220009327, 0.00345066050067544, 0.0037429295480251312, 0.003208654932677746, 0.003281221492215991] [0.0035543576814234257, 0.0035864568781107664, 0.003600216004997492, 0.003501922357827425, 0.003360101953148842, 0.003453771583735943, 0.0035035896580666304, 0.003432938829064369, 0.003479692619293928, 0.003323536366224289, 0.003620271570980549, 0.00368689838796854, 0.003532315604388714, 0.003665119642391801, 0.003431028686463833, 0.0034416173584759235, 0.0033111905213445425, 0.0035773171111941338, 0.003500281600281596, 0.003792376723140478, 0.0036410207394510508, 0.0033218367025256157, 0.0035101629327982664, 0.003480316372588277, 0.0035736244171857834, 0.0033623622730374336, 0.0033005070872604847, 0.003517939243465662, 0.003335624933242798, 0.0035660932771861553, 0.0035693440586328506, 0.0037171675357967615, 0.003516997443512082, 0.0035430456046015024, 0.0033861491829156876, 0.003484900575131178, 0.00366060808300972, 0.003699798369780183, 0.0034500176552683115, 0.0034008382353931665, 0.003346943063661456, 0.003391938516870141, 0.003391105681657791, 0.003779786638915539, 0.003443376161158085, 0.0035807674285024405, 0.003536819713190198, 0.0034459903836250305, 0.0034538607578724623, 0.003427310846745968, 0.0034935190342366695, 0.0035206901375204325, 0.0033992717508226633, 0.0034582288935780525, 0.003571025561541319, 0.0035783275961875916, 0.0034272833727300167, 0.0033782622776925564, 0.0037133696023374796, 0.0034392750822007656, 0.0034044459462165833, 0.003603878431022167, 0.0035674860700964928, 0.0034012822434306145, 0.003505145898088813, 0.003443656489253044, 0.0036405054852366447, 0.003523076418787241, 0.003681989386677742, 0.0032838331535458565, 0.003592395456507802, 0.0037093230057507753, 0.003433498088270426, 0.0033463402651250362, 0.0034004272893071175, 0.0035489737056195736, 0.003383481875061989, 0.003381048096343875, 0.003433539066463709, 0.0035415685269981623, 0.0037850947119295597, 0.003423110581934452, 0.0035701121669262648, 0.0034027514047920704, 0.003387174569070339, 0.003540076781064272, 0.003427306655794382, 0.003375506727024913, 0.0034578959457576275, 0.003276643343269825, 0.0034154988825321198, 0.0032801611814647913, 0.0033775330521166325, 0.0034224381670355797, 0.0034501415211707354, 0.003413296304643154, 0.003567189211025834, 0.003553384682163596, 0.0036039198748767376, 0.003443115623667836, 0.0034983092918992043, 0.0033931198995560408, 0.003411411540582776, 0.00346315442584455, 0.0033011268824338913, 0.0033177481964230537, 0.0036123809404671192, 0.0035764293279498816, 0.0036579081788659096, 0.0033568316139280796, 0.0034100350458174944, 0.0033372174948453903, 0.0034661060199141502, 0.0034244670532643795, 0.003372535575181246, 0.003534304443746805, 0.003615747205913067, 0.0035783727653324604, 0.003398351138457656, 0.0034683658741414547, 0.0036575093399733305, 0.0034084992948919535, 0.0035162093117833138, 0.0035625984892249107, 0.0034892200492322445, 0.003686979180201888, 0.0035344052594155073, 0.0034053768031299114, 0.0033632535487413406, 0.003537142649292946, 0.003580726683139801, 0.003229179186746478, 0.003207280533388257, 0.003288880456238985, 0.0034069721587002277, 0.0034656040370464325, 0.0035903877578675747, 0.0034654405899345875, 0.0033048493787646294, 0.0034911176189780235, 0.003481641411781311, 0.003613885957747698, 0.003468109294772148, 0.00344207719899714, 0.003220590762794018, 0.00334461429156363, 0.0033189570531249046, 0.0034650550223886967, 0.00339482887648046, 0.0034407740458846092, 0.003425898961722851, 0.0033072959631681442, 0.0034448187798261642, 0.0034645586274564266, 0.0033891352359205484, 0.00329886213876307, 0.0035939072258770466, 0.0034992611035704613, 0.0037549701519310474, 0.003634868422523141, 0.0034153489395976067, 0.0034473054111003876, 0.003688804805278778, 0.003262748708948493, 0.003323561744764447, 0.0037028412334620953, 0.003280195640400052, 0.0035149133764207363, 0.0034727167803794146, 0.0033501628786325455, 0.003422283101826906, 0.003305698512122035, 0.003573291003704071, 0.0034052692353725433, 0.0032531090546399355, 0.0035267327912151814, 0.003517827019095421, 0.0036666214000433683, 0.0033258746843785048, 0.0033337296918034554, 0.003651134669780731, 0.0031821068841964006, 0.0036602793261408806, 0.0034435309935361147, 0.0035069058649241924, 0.0034892891999334097, 0.003336023772135377, 0.0032862694934010506, 0.0036624502390623093, 0.003498108359053731, 0.0035505511332303286, 0.0034460765309631824, 0.0033836811780929565, 0.0034055947326123714, 0.003434370970353484, 0.003338407725095749, 0.003573562717065215, 0.0032364372164011, 0.003484415588900447, 0.0034884873311966658, 0.003598188515752554, 0.0033211528789252043, 0.003457456361502409, 0.0032677429262548685, 0.0034675649367272854, 0.0035509420558810234, 0.003503967309370637, 0.003302851226180792, 0.003370703663676977, 0.0033171731047332287, 0.0033808655571192503, 0.0033643264323472977, 0.0034224367700517178, 0.0035562971606850624, 0.0035733578260987997, 0.0033969643991440535, 0.0036048507317900658, 0.003519341815263033, 0.003467801958322525, 0.0036922891158610582, 0.003395435167476535, 0.0035437841434031725, 0.0033513400703668594, 0.003726063296198845, 0.0034564989618957043, 0.0033823892008513212, 0.0038042799569666386, 0.0037880046293139458, 0.003421521745622158, 0.0034777589607983828, 0.0032015203032642603, 0.0033990517258644104, 0.0034981886856257915, 0.003388423938304186, 0.003372070612385869, 0.0036308097187429667, 0.0035148439928889275, 0.0035153834614902735, 0.0035126081202179193, 0.003406156087294221, 0.0037513182032853365, 0.003492840565741062, 0.003277708077803254, 0.0037372992374002934, 0.0033944000024348497, 0.0034803859889507294, 0.0034764211159199476, 0.0034815403632819653, 0.0034830959048122168, 0.003623954253271222, 0.0034709947649389505, 0.0033337511122226715, 0.0032813844736665487, 0.0033923164010047913, 0.003643956035375595, 0.0032713026739656925, 0.003364664502441883, 0.0037291841581463814, 0.003551695728674531, 0.0033964295871555805, 0.003551218658685684, 0.0034080916084349155, 0.003366234712302685, 0.0034063723869621754, 0.0036508378107100725, 0.0035473164170980453, 0.0034298805985599756, 0.0036092582158744335, 0.003319897223263979, 0.0033650321420282125, 0.003563882317394018, 0.003591796150431037, 0.0033959972206503153, 0.0036090798676013947, 0.003452685661613941, 0.003460233798250556, 0.0033521701116114855, 0.003512235824018717, 0.0035472726449370384, 0.0036854997742921114, 0.0034474139101803303, 0.0034718364477157593, 0.003782253246754408, 0.0034301327541470528, 0.00341275567188859, 0.003295379690825939, 0.0033814169000834227, 0.0034588307607918978, 0.003480012994259596, 0.0032425166573375463, 0.0033408342860639095, 0.003371622646227479, 0.003571128472685814, 0.003464164212346077, 0.003349695820361376, 0.00341093260794878, 0.0033338216599076986, 0.0033610588870942593, 0.003537891199812293, 0.0033875955268740654, 0.0033441679552197456, 0.003372275736182928, 0.0033710955176502466, 0.0033865172881633043, 0.0033143330365419388, 0.003364504547789693, 0.003326983656734228, 0.0037260358221828938, 0.0033464175648987293, 0.003590030362829566, 0.0034220567904412746, 0.0034736262168735266, 0.0036110440269112587, 0.003355269320309162, 0.0032621119171380997, 0.0036199598107486963, 0.0031901560723781586, 0.0037520858459174633, 0.0034263196866959333, 0.0033850863110274076, 0.0036094337701797485, 0.0034780013374984264, 0.003587661776691675, 0.00352271506562829, 0.003382276277989149, 0.0034767519682645798, 0.0035053379833698273, 0.003460461739450693, 0.003479353152215481, 0.0034792383667081594, 0.0035845921374857426, 0.0036950705107301474, 0.0033641362097114325, 0.003500229911878705, 0.003620648290961981, 0.0036091466899961233, 0.003506729379296303, 0.0034956876188516617, 0.003498257603496313, 0.00333689758554101, 0.0036029531620442867, 0.0034547382965683937, 0.003332795575261116, 0.0034337276592850685, 0.0033801160752773285, 0.003552620531991124, 0.003572523593902588, 0.0032599971164017916, 0.0033630335237830877, 0.0035608368925750256, 0.0033219854813069105, 0.003237013937905431, 0.0036002332344651222, 0.0033058803528547287, 0.003459768835455179, 0.0034022012259811163, 0.0035549683962017298, 0.003638357622548938, 0.0033026558812707663, 0.003418716136366129, 0.003393398830667138, 0.0033561191521584988, 0.0035285381600260735, 0.00334091461263597, 0.00372642045840621, 0.0034686815924942493, 0.0032421182841062546, 0.003497479250654578, 0.0034463200718164444, 0.0033216800075024366, 0.0035986455623060465, 0.0034479014575481415, 0.0037104219663888216, 0.0033302849624305964, 0.003464057110249996, 0.00359444716013968, 0.003316287649795413, 0.003331863321363926, 0.003285761456936598, 0.0033136336132884026, 0.003599950112402439, 0.0034743358846753836, 0.003446305403485894, 0.0034342268481850624, 0.0034308042377233505, 0.003412802703678608, 0.0034895320422947407, 0.003526747226715088, 0.003485244931653142, 0.0035444730892777443, 0.0035311407409608364, 0.003439545165747404, 0.003582356497645378, 0.003167862305417657, 0.003514759009703994, 0.0034032303374260664, 0.003731467993929982, 0.003490571631118655, 0.003489531110972166, 0.0035592929925769567, 0.0034916175063699484, 0.0035402977373450994, 0.0032868715934455395, 0.0032199397683143616, 0.0033245424274355173, 0.003424392081797123, 0.0033412298653274775, 0.003441793378442526, 0.0035364991053938866, 0.0035518715158104897, 0.003505655098706484, 0.003368816338479519, 0.0035424421075731516, 0.003315168432891369, 0.00354919396340847, 0.003548769745975733, 0.0034263115376234055, 0.0034555154852569103, 0.0036679969634860754, 0.0035232645459473133, 0.0035258796997368336, 0.0036751143634319305, 0.0036070202477276325, 0.003516060533002019, 0.003326928708702326, 0.00354611873626709, 0.0033126487396657467, 0.0036608497612178326, 0.0033453735522925854, 0.0034101407509297132, 0.0034301390405744314, 0.0032520012464374304, 0.003437866922467947, 0.0035273332614451647, 0.003435293212532997, 0.0035455795004963875, 0.003571918932721019, 0.003317411057651043, 0.0035196314565837383, 0.003694649785757065, 0.0034287311136722565, 0.0034520714543759823, 0.0034147226251661777, 0.0032976700458675623, 0.003484611865133047, 0.003513620002195239, 0.0035652273800224066, 0.0035369060933589935, 0.003334734123200178, 0.0035405242815613747, 0.003494145115837455, 0.0033193230628967285, 0.0034114946611225605, 0.0033571249805390835, 0.0033746245317161083, 0.0033153770491480827, 0.0033330449368804693, 0.003399459645152092, 0.003288203850388527, 0.0033829370513558388, 0.0036975289694964886, 0.003445529844611883, 0.003740154206752777, 0.003534805029630661, 0.003449441399425268, 0.00336672505363822, 0.0037313243374228477, 0.003322318894788623, 0.003480583429336548, 0.003623540513217449, 0.0033786091953516006, 0.003357733367010951, 0.003361692652106285, 0.003424421651288867, 0.0036755758337676525, 0.003345157951116562, 0.0035163976717740297, 0.0035869425628334284, 0.0036838168743997812, 0.003410852514207363, 0.0036541768349707127, 0.0034809536300599575, 0.003455085214227438, 0.003444364294409752, 0.0034203934483230114, 0.0035885912366211414, 0.003499033395200968, 0.003404076676815748, 0.003632132662460208, 0.0033113406971096992, 0.0036050602793693542, 0.0033692577853798866, 0.0036369599401950836, 0.0033744466491043568, 0.0034254849888384342, 0.0035696113482117653, 0.0036417110823094845, 0.003502837149426341, 0.00324605917558074, 0.0033903024159371853, 0.003357202047482133, 0.0036547607742249966, 0.0034142821095883846, 0.0033069325145334005, 0.003465548623353243, 0.0035502100363373756, 0.0036021550185978413, 0.0035376441664993763, 0.003457052167505026, 0.00337745388969779, 0.0035385671071708202, 0.0033827535808086395, 0.003695870516821742, 0.0035306287463754416, 0.0035005472600460052, 0.003523994004353881, 0.0034440006129443645, 0.003631522413343191, 0.003560837125405669, 0.003340377239510417, 0.00355124706402421, 0.003464306937530637, 0.003614421933889389, 0.0035402472130954266, 0.0033348798751831055, 0.003488037968054414, 0.0033603538759052753, 0.0033609759993851185, 0.003437262261286378, 0.003577051917091012, 0.0033667038660496473, 0.003432125085964799, 0.003455406753346324, 0.0036683063954114914, 0.00346684199757874, 0.003474802477285266, 0.003424711525440216, 0.003718210384249687, 0.0032692495733499527, 0.0035927733406424522, 0.003263341262936592, 0.0034239650703966618, 0.003466709516942501, 0.00342510174959898, 0.0033557116985321045, 0.003569855121895671, 0.0036770356819033623, 0.003334461245685816, 0.003703724592924118, 0.00330156646668911, 0.0034522716887295246, 0.0034896277356892824, 0.003371611936017871, 0.003485528752207756, 0.0036948849447071552, 0.0032858652994036674, 0.0035490221343934536, 0.0035830666311085224, 0.0035545234568417072, 0.003324743825942278, 0.0033384351991117, 0.0035129147581756115, 0.00339136878028512, 0.0032874997705221176, 0.0038298452273011208, 0.0038337528239935637, 0.0034734560176730156, 0.003531539812684059, 0.0037134368903934956, 0.0035095943603664637, 0.0035425859969109297, 0.0033587226644158363, 0.0036413935013115406, 0.0032767807133495808, 0.003478744998574257, 0.0034448732621967793, 0.0033725069370120764, 0.003343602642416954, 0.003830085741356015, 0.0034587127156555653, 0.003432394005358219, 0.0034460541792213917, 0.0036269407719373703, 0.0034634924959391356, 0.0033540818840265274, 0.0033976463600993156, 0.003528641304001212, 0.0034651097375899553, 0.0035464754328131676, 0.0035536077339202166, 0.0034534570295363665, 0.0034134734887629747, 0.003403929527848959, 0.003440194297581911, 0.0034179161302745342, 0.0034888237714767456, 0.003279712051153183, 0.003592617344111204, 0.0033244288060814142, 0.0036993841640651226, 0.003401582594960928, 0.003298624884337187, 0.003600775497034192, 0.00376694742590189, 0.0034129521809518337, 0.0032876315526664257, 0.003407606389373541, 0.0033372403122484684, 0.0034197703935205936, 0.0035035249311476946, 0.0034730820916593075, 0.0033415532670915127, 0.0034522749483585358, 0.0034543094225227833, 0.0033374286722391844, 0.003456573002040386, 0.0033971171360462904, 0.003311087377369404, 0.003733145073056221, 0.0034040119498968124, 0.003476198296993971, 0.0034464302007108927, 0.003243661718443036, 0.0032982523553073406, 0.003504449501633644, 0.0034575704485177994, 0.003648458980023861, 0.0035229665227234364, 0.0035196924582123756, 0.003616279922425747, 0.0032718724105507135, 0.003427937626838684, 0.0034346021711826324, 0.0035199667327106, 0.003389817662537098, 0.003533236449584365, 0.003604427445679903, 0.0032434286549687386, 0.0036867684684693813, 0.0035531793255358934, 0.0035500135272741318, 0.0033557815477252007, 0.003721887245774269, 0.0035498617216944695, 0.0033931604120880365, 0.0036578672006726265, 0.0032360516488552094, 0.0034042124170809984, 0.0035342408809810877, 0.0033909636549651623, 0.0032980432733893394, 0.003410062287002802, 0.003479010658338666, 0.0033551512751728296, 0.0035837357863783836, 0.0033760922960937023, 0.003329487517476082, 0.0033546998165547848, 0.0033382053952664137, 0.003593794070184231, 0.0033443684224039316, 0.0032991899643093348, 0.003701609093695879, 0.0033578570000827312, 0.003571191569790244, 0.0035493038594722748, 0.003435279708355665, 0.0033859042450785637, 0.0034650613088160753, 0.0034401961602270603, 0.0035459334030747414, 0.00360533082857728, 0.003377330955117941, 0.003630605759099126, 0.0036635720171034336, 0.003658880479633808, 0.0036174331326037645, 0.003372910898178816, 0.0032565307337790728, 0.0035152710042893887, 0.0033317701891064644, 0.0034986594691872597, 0.003522583981975913, 0.0033702622167766094, 0.0034092823043465614, 0.0038135910872370005, 0.0033959653228521347, 0.0035372101701796055, 0.0033897305838763714, 0.003412130055949092, 0.003383238799870014, 0.0034638643264770508, 0.003435746068134904, 0.0035200805868953466, 0.0036505362950265408, 0.0032318588346242905, 0.0035286801867187023, 0.003391214646399021, 0.003524352563545108, 0.0037809975910931826, 0.003543015569448471, 0.0033685090020298958, 0.003379570320248604, 0.0035477671772241592, 0.0033827805891633034, 0.003519379533827305, 0.0034096946474164724, 0.0034845576155930758, 0.0033464315347373486, 0.003253998002037406, 0.003458837978541851, 0.0035954941995441914, 0.003375652013346553, 0.0032290536910295486, 0.003439526539295912, 0.0033575615379959345, 0.003485143417492509, 0.0035589183680713177, 0.0032910797744989395, 0.0033291708678007126, 0.0037170376162976027, 0.003488232847303152, 0.0035780146718025208, 0.0035944771952927113, 0.0034439859446138144, 0.0033952868543565273, 0.003515357617288828, 0.0035399473272264004, 0.003411596640944481, 0.003362887306138873, 0.0033395332284271717, 0.003517487086355686, 0.0035872238222509623, 0.0032623899169266224, 0.0033335399348288774, 0.003301670541986823, 0.003408783581107855, 0.0035892014857381582, 0.003449562704190612, 0.003659318434074521, 0.003525999141857028, 0.0032820808701217175, 0.0034463624469935894, 0.0034252700861543417, 0.0035852736327797174, 0.003291215281933546, 0.00359382014721632, 0.0035727841313928366, 0.003347872756421566, 0.003388739190995693, 0.0035753180272877216, 0.0035638229455798864, 0.003358186688274145, 0.0036174326669424772, 0.0036200121976435184, 0.0033340342342853546, 0.003449514973908663, 0.0035055724438279867, 0.003668674500659108, 0.003404805436730385, 0.0034979735501110554, 0.003456555772572756, 0.003352492582052946, 0.00357822235673666, 0.0034755943343043327, 0.0032328255474567413, 0.003343184944242239, 0.003411265555769205, 0.0034556984901428223, 0.003553601913154125, 0.0034429647494107485, 0.0036125159822404385, 0.003442535176873207, 0.0036327503621578217, 0.0033466508612036705, 0.0034016473218798637, 0.003728373907506466, 0.0035935798659920692, 0.003499856684356928, 0.003507656278088689, 0.00335556548088789, 0.0035359712783247232] [0.0034815692342817783, 0.0036601528991013765, 0.003388077486306429, 0.0034773219376802444, 0.003536052769050002, 0.003781696781516075, 0.0034378264099359512, 0.003399706445634365, 0.0032976497896015644, 0.0035149105824530125, 0.0034557590261101723, 0.0035966364666819572, 0.0034091342240571976, 0.0036550506483763456, 0.0033914351370185614, 0.0036255265586078167, 0.003498417790979147, 0.003496866673231125, 0.003121138084679842, 0.0034372410736978054, 0.0033287073019891977, 0.0033794851042330265, 0.003421033965423703, 0.003380216658115387, 0.00322912260890007, 0.0035775937139987946, 0.0031724118161946535, 0.003485830733552575, 0.0034538256004452705, 0.003483367385342717, 0.003335681278258562, 0.003456174861639738, 0.0033754846081137657, 0.0033369911834597588, 0.0036579621955752373, 0.0034692552872002125, 0.0036224627401679754, 0.003324122168123722, 0.0033935154788196087, 0.003493125317618251, 0.003275689436122775, 0.0032776091247797012, 0.0034884882625192404, 0.003436499275267124, 0.0033698352053761482, 0.0034609264694154263, 0.0035584340803325176, 0.0033412829507142305, 0.003346778452396393, 0.003421326167881489, 0.0033151765819638968, 0.00360401114448905, 0.003602909157052636, 0.0034783415030688047, 0.0036303973756730556, 0.0033736927434802055, 0.0033331960439682007, 0.0034237366635352373, 0.0034463629126548767, 0.0033059967681765556, 0.003580264048650861, 0.0033681406639516354, 0.0033566064666956663, 0.003470293479040265, 0.003522353246808052, 0.003485750872641802, 0.0036466247402131557, 0.003377346321940422, 0.0034367251209914684, 0.0033602402545511723, 0.003395476145669818, 0.0033057108521461487, 0.003387406002730131, 0.003506109584122896, 0.003476951736956835, 0.003395501058548689, 0.0031766085885465145, 0.003572342451661825, 0.003489496186375618, 0.003299275878816843, 0.003360535018146038, 0.0033867068123072386, 0.003757310565561056, 0.0034487226512283087, 0.0036262308713048697, 0.0035131487529724836, 0.003437195671722293, 0.0034049476962536573, 0.0032291144598275423, 0.00343895610421896, 0.0032080612145364285, 0.003657981753349304, 0.003276502713561058, 0.0034077262971550226, 0.0032886227127164602, 0.003469266463071108, 0.0036361166276037693, 0.0035284673795104027, 0.003455521771684289, 0.0036748056299984455, 0.00357202161103487, 0.003463977249339223, 0.003608717815950513, 0.0034107817336916924, 0.0033951716031879187, 0.003639772068709135, 0.0034251688048243523, 0.003296141978353262, 0.003785286797210574, 0.0034964275546371937, 0.003519458696246147, 0.0034672494512051344, 0.003475104458630085, 0.0034156152978539467, 0.003357682842761278, 0.0032520468812435865, 0.003444656264036894, 0.00344413286074996, 0.003425589529797435, 0.0035213688388466835, 0.003557969117537141, 0.003376677166670561, 0.0034157412592321634, 0.0033918479457497597, 0.0034366229083389044, 0.00374644435942173, 0.0034636077471077442, 0.0034144441597163677, 0.0034887189976871014, 0.0034047116059809923, 0.0035542272962629795, 0.003606307553127408, 0.003318886971101165, 0.003400926012545824, 0.0034290682524442673, 0.0034932491835206747, 0.0034867185167968273, 0.0034485813230276108, 0.0035763573832809925, 0.0033624134957790375, 0.003233129158616066, 0.0035625684540718794, 0.0032400779891759157, 0.0033965744078159332, 0.0034973518922924995, 0.003531363559886813, 0.0031391438096761703, 0.0034921998158097267, 0.0034773978404700756, 0.003592386841773987, 0.003490815171971917, 0.003460186067968607, 0.0034894824493676424, 0.0036279242485761642, 0.0034431309904903173, 0.003210417227819562, 0.0035169445909559727, 0.0034756490495055914, 0.003519665915518999, 0.0034414255060255527, 0.0032991385087370872, 0.003405005671083927, 0.0033722100779414177, 0.0033450094051659107, 0.003397052176296711, 0.003640321083366871, 0.0034851159434765577, 0.003525722771883011, 0.00348791666328907, 0.0033863047137856483, 0.0035373184364289045, 0.003561905585229397, 0.0034407847560942173, 0.003461559070274234, 0.003760427003726363, 0.0033779810182750225, 0.0032999541144818068, 0.003558902069926262, 0.0033559189178049564, 0.003404013579711318, 0.0032892529852688313, 0.003433548379689455, 0.0035296170972287655, 0.003577600931748748, 0.0033089325297623873, 0.0034135086461901665, 0.003623845987021923, 0.003534768708050251, 0.00328013114631176, 0.003412010380998254, 0.0034137109760195017, 0.0036280332133173943, 0.0033972696401178837, 0.003606226062402129, 0.0036355832125991583, 0.0034533098805695772, 0.0036428633611649275, 0.003496837569400668, 0.0036043154541403055, 0.0034224274568259716, 0.0035683908499777317, 0.0035658071283251047, 0.003508062334731221, 0.0034874817356467247, 0.003557186108082533, 0.0033845813013613224, 0.003558186814188957, 0.0034429985098540783, 0.0035088222939521074, 0.0035574864596128464, 0.0036123893223702908, 0.0034177592024207115, 0.003549236338585615, 0.0034952396526932716, 0.003314095549285412, 0.0034477664157748222, 0.0036030039191246033, 0.0035615824162960052, 0.00353069044649601, 0.0035048527643084526, 0.0033368489239364862, 0.003622319083660841, 0.003422016743570566, 0.003523191437125206, 0.0035147019661962986, 0.003619883442297578, 0.0035145918373018503, 0.003375341882929206, 0.003498186357319355, 0.003431966993957758, 0.003637368092313409, 0.0033934677485376596, 0.0033232911955565214, 0.0033273338340222836, 0.0036121197044849396, 0.0035266070626676083, 0.0033669599797576666, 0.0032795718871057034, 0.0032456577755510807, 0.003487007925286889, 0.0036559244617819786, 0.00347620015963912, 0.003456593956798315, 0.0033432496711611748, 0.0032228233758360147, 0.0033301040530204773, 0.0032537588849663734, 0.003357389010488987, 0.0032420549541711807, 0.0035871402360498905, 0.0034103996586054564, 0.0034925590734928846, 0.0034936144948005676, 0.0034262961708009243, 0.0034849869552999735, 0.0035360592883080244, 0.0034054075367748737, 0.003572569228708744, 0.003695694962516427, 0.003426658920943737, 0.0032992481719702482, 0.0033850602339953184, 0.003533522365614772, 0.0031859998125582933, 0.0035398011095821857, 0.003348382655531168, 0.003335774876177311, 0.003469988703727722, 0.0034089423716068268, 0.0029898243956267834, 0.003466247580945492, 0.0034821880981326103, 0.003385729854926467, 0.003501097671687603, 0.0035249078646302223, 0.0035363943316042423, 0.003579392097890377, 0.0037743588909506798, 0.00363335944712162, 0.003418182721361518, 0.003279850585386157, 0.0033626812510192394, 0.0034483736380934715, 0.0034226570278406143, 0.0037700566463172436, 0.0034732201602309942, 0.0033980486914515495, 0.003535213880240917, 0.0036353711038827896, 0.0034335656091570854, 0.003331368323415518, 0.0035775299184024334, 0.003559570759534836, 0.003236017655581236, 0.003353765467181802, 0.0036095581017434597, 0.0032965359278023243, 0.003522477112710476, 0.0034417035058140755, 0.0035866298712790012, 0.003432466648519039, 0.003232837188988924, 0.0033763665705919266, 0.003385741962119937, 0.0036459765397012234, 0.0035484679974615574, 0.0034630566369742155, 0.0032983305864036083, 0.003359519876539707, 0.003408577758818865, 0.003574908943846822, 0.0035527306608855724, 0.003642681986093521, 0.0034730008337646723, 0.0034569352865219116, 0.0036069725174456835, 0.003436049446463585, 0.003448234871029854, 0.0032598907127976418, 0.003507077693939209, 0.0035942867398262024, 0.003473251359537244, 0.0033811875618994236, 0.003722197376191616, 0.003228598739951849, 0.0033804108388721943, 0.0034799622371792793, 0.0032398123294115067, 0.003462402615696192, 0.003623706055805087, 0.003286992432549596, 0.003316364949569106, 0.0036777430213987827, 0.003568794811144471, 0.0034359898418188095, 0.00341345788910985, 0.0034183766692876816, 0.0034040547907352448, 0.0036954358220100403, 0.0034522530622780323, 0.003673880361020565, 0.00323493336327374, 0.003579007927328348, 0.003213883377611637, 0.00333270151168108, 0.003452291712164879, 0.003455153200775385, 0.0036855475045740604, 0.003375535598024726, 0.0035419794730842113, 0.0035203807055950165, 0.00335658504627645, 0.0035431638825684786, 0.0032149411272257566, 0.003454207442700863, 0.003317404305562377, 0.0034416834823787212, 0.003337982576340437, 0.003597792936488986, 0.0032319119200110435, 0.003417978063225746, 0.0034529687836766243, 0.00359911285340786, 0.00360314198769629, 0.003632716368883848, 0.0035672876983880997, 0.003448900766670704, 0.0033504394814372063, 0.0033447938039898872, 0.003405509050935507, 0.0033534138929098845, 0.0034008834045380354, 0.003263568039983511, 0.0033721006475389004, 0.0035021400544792414, 0.0032831274438649416, 0.00355666340328753, 0.0036384952254593372, 0.0033989381045103073, 0.0035505052655935287, 0.0035116979852318764, 0.0035283700563013554, 0.0034934731665998697, 0.0035018185153603554, 0.003302539698779583, 0.0033793048933148384, 0.003341927658766508, 0.0033560527954250574, 0.003708413802087307, 0.0034220251254737377, 0.003369841491803527, 0.003525653388351202, 0.0036712547298520803, 0.0034066501539200544, 0.00351023580878973, 0.003388437908142805, 0.0034379512071609497, 0.0034231473691761494, 0.00348484399728477, 0.0037950207479298115, 0.003521684091538191, 0.0032681303564459085, 0.003268282860517502, 0.0032469402067363262, 0.0035486326087266207, 0.003563122358173132, 0.003394986502826214, 0.003526396816596389, 0.0034345807507634163, 0.0035589851904660463, 0.003520822850987315, 0.0033510723151266575, 0.003625604324042797, 0.0032666483893990517, 0.003385918214917183, 0.003608454018831253, 0.003351864404976368, 0.003358082380145788, 0.0033259589690715075, 0.003494792152196169, 0.00342353293672204, 0.0036301729269325733, 0.003520299680531025, 0.003511639777570963, 0.003449762472882867, 0.003526927437633276, 0.0034544835798442364, 0.0033507738262414932, 0.003445205744355917, 0.0034149247221648693, 0.0033785912673920393, 0.003414345206692815, 0.003514245618134737, 0.00339104188606143, 0.0036246338859200478, 0.0034480260219424963, 0.0035590450279414654, 0.003456410486251116, 0.003310424042865634, 0.0033711392898112535, 0.003298138501122594, 0.0035202372819185257, 0.003396493149921298, 0.0035725152119994164, 0.003522395621985197, 0.0033255277667194605, 0.0033224672079086304, 0.003621765412390232, 0.0035125187132507563, 0.003643415402621031, 0.003642420982941985, 0.0034322617575526237, 0.0034881962928920984, 0.00342250382527709, 0.0035235672257840633, 0.003508160822093487, 0.0037333567161113024, 0.00355307525023818, 0.0034500323235988617, 0.0036179497838020325, 0.003549660323187709, 0.003325078636407852, 0.0034543247893452644, 0.0034684552811086178, 0.00375386793166399, 0.0033618039451539516, 0.0034772721119225025, 0.0031604785472154617, 0.003508506342768669, 0.0035077324137091637, 0.0035710930824279785, 0.003310946747660637, 0.003534857649356127, 0.003262922866269946, 0.0033734100870788097, 0.003354814602062106, 0.0036460014525800943, 0.0033824981655925512, 0.0034268582239747047, 0.00349314883351326, 0.0034087924286723137, 0.003396372776478529, 0.0035637442488223314, 0.0034153256565332413, 0.0035183506552129984, 0.003549417480826378, 0.0034701998811215162, 0.0035393601283431053, 0.0034947015810757875, 0.003454612335190177, 0.0033868697937577963, 0.003395343665033579, 0.0036024090368300676, 0.003520594909787178, 0.0034424467012286186, 0.003503388725221157, 0.003546067513525486, 0.003664554562419653, 0.0037115029990673065, 0.003575578797608614, 0.003565443679690361, 0.003400046145543456, 0.0036927424371242523, 0.0035047256387770176, 0.0034622694365680218, 0.003440038999542594, 0.003497662488371134, 0.0035363496281206608, 0.0037216302007436752, 0.003414479549974203, 0.003397599793970585, 0.0034456446301192045, 0.003469366580247879, 0.0034887674264609814, 0.0034071828704327345, 0.0033581440802663565, 0.0035250321961939335, 0.003256636206060648, 0.0036558606661856174, 0.00353914313018322, 0.0035459273494780064, 0.0033109544310718775, 0.003228438086807728, 0.0035069803707301617, 0.003385646501556039, 0.0034738508984446526, 0.003408813616260886, 0.003608142025768757, 0.0035678641870617867, 0.003624631790444255, 0.003646673634648323, 0.003439646679908037, 0.0034963060170412064, 0.003534529358148575, 0.003264555474743247, 0.0034164516255259514, 0.00341154751367867, 0.0035241914447396994, 0.003524443367496133, 0.0034352270886301994, 0.003290514461696148, 0.0034124760422855616, 0.003661138005554676, 0.003382619470357895, 0.003596288152039051, 0.0033739646896719933, 0.0034859732259064913, 0.0034928913228213787, 0.0033990435767918825, 0.0037510436959564686, 0.0037428056821227074, 0.0035470204893499613, 0.003477228805422783, 0.00365844601765275, 0.0034140341449528933, 0.0033950861543416977, 0.0034398785792291164, 0.003440835978835821, 0.003455758560448885, 0.003283100202679634, 0.0035345640499144793, 0.0034618948120623827, 0.003641876159235835, 0.0033458031248301268, 0.0034593690652400255, 0.003512216731905937, 0.0036829751916229725, 0.0036073527298867702, 0.003385968506336212, 0.003244078252464533, 0.003672479186207056, 0.0034745209850370884, 0.0033863275311887264, 0.003488980233669281, 0.0033334929030388594, 0.0034347844775766134, 0.0033339299261569977, 0.0035062027163803577, 0.0034659174270927906, 0.003489874070510268, 0.0034446255303919315, 0.00350860133767128, 0.003467496484518051, 0.0036634993739426136, 0.003536822507157922, 0.0036049780901521444, 0.00353932473808527, 0.0034929830580949783, 0.0035294191911816597, 0.003540647681802511, 0.0034328228794038296, 0.003475292818620801, 0.00329625909216702, 0.0031683288980275393, 0.0036141553428024054, 0.00352268572896719, 0.00344836269505322, 0.003376810345798731, 0.0035313917323946953, 0.003251048270612955, 0.003456033067777753, 0.0033647227101027966, 0.003575351554900408, 0.003532980103045702, 0.003422473557293415, 0.0036548320204019547, 0.0036102556623518467, 0.003287786617875099, 0.0034161722287535667, 0.003393883816897869, 0.003526080399751663, 0.0033355874475091696, 0.0032639051787555218, 0.003603881224989891, 0.003583610989153385, 0.0034046319779008627, 0.0034209243021905422, 0.0033009881153702736, 0.0035828757099807262, 0.003389006946235895, 0.0037751703057438135, 0.0034632666502147913, 0.0034521357156336308, 0.0034220111556351185, 0.00353059615008533, 0.003672313876450062, 0.00359873054549098, 0.003309571649879217, 0.003439968451857567, 0.003511134535074234, 0.0035662848968058825, 0.0036161099560558796, 0.003490857547149062, 0.0037294288631528616, 0.003403645008802414, 0.003516735974699259, 0.0035432851873338223, 0.0035439368803054094, 0.0034931048285216093, 0.003617749782279134, 0.003556677605956793, 0.0034667649306356907, 0.0035751499235630035, 0.003435968654230237, 0.0034048925153911114, 0.0034247206058353186, 0.0037248418666422367, 0.003667717333883047, 0.0034164011012762785, 0.0034689700696617365, 0.0035148910246789455, 0.0034423787146806717, 0.003424721537157893, 0.0036002383567392826, 0.003592078574001789, 0.003498809179291129, 0.003566608764231205, 0.003355074441060424, 0.0037530262488871813, 0.0037045301869511604, 0.0032556033693253994, 0.0035241758450865746, 0.003418844658881426, 0.0035580319818109274, 0.003500038292258978, 0.0034572253935039043, 0.003530750749632716, 0.0032366300001740456, 0.0034987647086381912, 0.0034377260599285364, 0.0034394427202641964, 0.00343575282022357, 0.003425582777708769, 0.0036118952557444572, 0.003495224518701434, 0.0035804398357868195, 0.0035010729916393757, 0.0035463841632008553, 0.003292629262432456, 0.003296290524303913, 0.0034424220211803913, 0.003408511634916067, 0.0032866476103663445, 0.003531110007315874, 0.0034107959363609552, 0.003547719679772854, 0.003314484842121601, 0.003484459361061454, 0.003526214975863695, 0.003239960875362158, 0.0035697463899850845, 0.0033790348097682, 0.0032295724377036095, 0.0035556654911488295, 0.003587951185181737, 0.0035448563285171986, 0.0033894202206283808, 0.003750602947548032, 0.0035804593935608864, 0.0037350403144955635, 0.0034909804817289114, 0.0035602268762886524, 0.0033643878996372223, 0.003563420847058296, 0.003522205166518688, 0.0032198242843151093, 0.003738891799002886, 0.0034190344158560038, 0.0035338299348950386, 0.003566513769328594, 0.0033560795709490776, 0.0035294382832944393, 0.0036654218565672636, 0.003586428938433528, 0.003400229150429368, 0.0034864619374275208, 0.0034446679055690765, 0.003390191588550806, 0.003405109280720353, 0.0034587038680911064, 0.0034010554663836956, 0.0033327850978821516, 0.003399200737476349, 0.0035160062834620476, 0.0035063158720731735, 0.0034197745844721794, 0.0035609398037195206, 0.003362910822033882, 0.003695917082950473, 0.003717097220942378, 0.0032605479937046766, 0.003739754669368267, 0.003376643406227231, 0.0034984881058335304, 0.003253708127886057, 0.003524523228406906, 0.003350277431309223, 0.0036512380465865135, 0.0035140584222972393, 0.0034678024239838123, 0.003492322051897645, 0.003437603125348687, 0.0035808582324534655, 0.0033904965966939926, 0.00356388371437788, 0.0034436453133821487, 0.0036890278570353985, 0.003373643383383751, 0.0035201068967580795, 0.0033379888627678156, 0.0035318732261657715, 0.0035163499414920807, 0.0033371667377650738, 0.0035767678637057543, 0.0035696495324373245, 0.0035915018524974585, 0.0034875257406383753, 0.003356397617608309, 0.0033886064775288105, 0.0034783300943672657, 0.0036386020947247744, 0.0035411068238317966, 0.0033285492099821568, 0.0034196842461824417, 0.0034108031541109085, 0.0035696031991392374, 0.0035223879385739565, 0.0035201208665966988, 0.003459836123511195, 0.003720882348716259, 0.003519779536873102, 0.0034552952274680138, 0.003505242522805929, 0.0034143871162086725, 0.0034948268439620733, 0.0036223477218300104, 0.0034208614379167557, 0.003356537315994501, 0.003505149856209755, 0.003726584604009986, 0.0033610942773520947, 0.003634174820035696] [0.003323805285617709, 0.00359505251981318, 0.0034799273125827312, 0.0036421746481209993, 0.0034282293636351824, 0.003307139966636896, 0.0036435737274587154, 0.0035151017364114523, 0.0037236958742141724, 0.003612895729020238, 0.00351908290758729, 0.003478323807939887, 0.0035042499657720327, 0.0034246777649968863, 0.003657570807263255, 0.0035296042915433645, 0.003392639337107539, 0.003243626095354557, 0.003590810112655163, 0.003373916493728757, 0.003721322864294052, 0.003422975307330489, 0.0033471875358372927, 0.0035266762133687735, 0.0033491929061710835, 0.0037010428495705128, 0.0034827939234673977, 0.0034906468354165554, 0.0034621339291334152, 0.0034523122012615204, 0.0032881153747439384, 0.0034788260236382484, 0.003527233377099037, 0.003402186557650566, 0.0033146762289106846, 0.0034061307087540627, 0.0034035928547382355, 0.0034910254180431366, 0.003451200434938073, 0.0035874764434993267, 0.0035348760429769754, 0.0033313781023025513, 0.00341050885617733, 0.0033841999247670174, 0.0033667301759123802, 0.003390233963727951, 0.0032950344029814005, 0.0034572638105601072, 0.003529045730829239, 0.0033998540602624416, 0.0035036392509937286, 0.0034601176157593727, 0.003390521975234151, 0.003410495351999998, 0.0034658852964639664, 0.003448385978117585, 0.003554227761924267, 0.0034223822876811028, 0.0034675393253564835, 0.003603870514780283, 0.00341884046792984, 0.0032796738669276237, 0.003512017894536257, 0.0035019326023757458, 0.0035768940579146147, 0.0035514445044100285, 0.003645802615210414, 0.003277990035712719, 0.0033045653253793716, 0.0033064146991819143, 0.0034205361735075712, 0.003646094584837556, 0.003339420072734356, 0.0034385151229798794, 0.0035527460277080536, 0.003700622823089361, 0.003579290583729744, 0.003571387380361557, 0.0035582990385591984, 0.0032478433568030596, 0.0034361935686320066, 0.0034978396724909544, 0.0033817519433796406, 0.003474834142252803, 0.003403295064345002, 0.0034002342727035284, 0.0034087179228663445, 0.0033656973391771317, 0.0034845536574721336, 0.003411587094888091, 0.00352804409340024, 0.0033580854069441557, 0.003409815952181816, 0.0035687657073140144, 0.003547096159309149, 0.0035116979852318764, 0.0034890456590801477, 0.0032953766640275717, 0.0034833105746656656, 0.0034252621699124575, 0.0037814180832356215, 0.0034083425998687744, 0.0034393337555229664, 0.003522624960169196, 0.00323331612162292, 0.0033250870183110237, 0.0037038398440927267, 0.0035828961990773678, 0.003427972085773945, 0.0032892758026719093, 0.003469963790848851, 0.003646282246336341, 0.003679827321320772, 0.003400645451620221, 0.0033752392046153545, 0.0033406559377908707, 0.003560767974704504, 0.0034254814963787794, 0.0033991774544119835, 0.0033405511640012264, 0.0035362569615244865, 0.0033394142519682646, 0.003660375950857997, 0.0033504865132272243, 0.003417843021452427, 0.0034646065905690193, 0.00363685330376029, 0.003381644608452916, 0.0033657625317573547, 0.0034895569551736116, 0.003811065573245287, 0.00350552168674767, 0.0035610119812190533, 0.003523835213854909, 0.003454220946878195, 0.003488364862278104, 0.0034474425483494997, 0.0035838012117892504, 0.003651207312941551, 0.0032518920488655567, 0.0035013610031455755, 0.003419457236304879, 0.0033894283697009087, 0.0033655324950814247, 0.0032823835499584675, 0.003497167956084013, 0.003312099492177367, 0.0034077244345098734, 0.003537387354299426, 0.0034366571344435215, 0.0033839214593172073, 0.0034401307348161936, 0.003594938199967146, 0.0033134697005152702, 0.003623112803325057, 0.0034481880720704794, 0.003564996412023902, 0.0034187184646725655, 0.0033565140329301357, 0.003340860828757286, 0.003562348894774914, 0.003273427952080965, 0.003411976620554924, 0.0032813420984894037, 0.00354215782135725, 0.003813986899331212, 0.0032581952400505543, 0.003348139114677906, 0.0036477006506174803, 0.003521377220749855, 0.0033500378485769033, 0.003319910727441311, 0.0032314767595380545, 0.003527048509567976, 0.0034572575241327286, 0.0035139531828463078, 0.003422833513468504, 0.003661004826426506, 0.0033863200806081295, 0.0035094486083835363, 0.0036029741168022156, 0.0034896736033260822, 0.0034883792977780104, 0.0034482814371585846, 0.0035223253071308136, 0.003622008254751563, 0.003480468410998583, 0.003540562465786934, 0.0032769530080258846, 0.003349164268001914, 0.0032989715691655874, 0.0032406803220510483, 0.0034015525598078966, 0.0033335709013044834, 0.0032758803572505713, 0.0034917916636914015, 0.003606347367167473, 0.0035855737514793873, 0.0034213762264698744, 0.003450662363320589, 0.003547530621290207, 0.0033951669465750456, 0.0035004219971597195, 0.00325587997213006, 0.0034946182277053595, 0.0034876600839197636, 0.003553912043571472, 0.0036418370436877012, 0.0037206772249192, 0.0035452658776193857, 0.0034832507371902466, 0.0034266854636371136, 0.0034506439696997404, 0.003408250166103244, 0.0034193897154182196, 0.0034289404284209013, 0.003554766997694969, 0.003484731540083885, 0.0033075218088924885, 0.0034444343764334917, 0.0034021218307316303, 0.0036128219217061996, 0.003348465310409665, 0.0034758076071739197, 0.0033515682443976402, 0.0033248253166675568, 0.0033525070175528526, 0.0034344722516834736, 0.0036464740987867117, 0.00359682971611619, 0.0035249204374849796, 0.003518503624945879, 0.0034250810276716948, 0.0033535079564899206, 0.0035376427695155144, 0.00352768087759614, 0.00332074286416173, 0.003536076517775655, 0.0035533986520022154, 0.003370469668880105, 0.0034056897275149822, 0.0035499553196132183, 0.003508264198899269, 0.0036019664257764816, 0.0035689871292561293, 0.003377708839252591, 0.0034275271464139223, 0.0035747289657592773, 0.0032663107849657536, 0.003460926003754139, 0.003564810147508979, 0.003435781691223383, 0.0037172618322074413, 0.0035023014061152935, 0.003125168848782778, 0.003461721818894148, 0.0032992796041071415, 0.0034007341600954533, 0.003326434176415205, 0.003450998105108738, 0.003618070390075445, 0.0034276123624294996, 0.0036222320050001144, 0.0036733592860400677, 0.003588030580431223, 0.0035827788524329662, 0.0032873242162168026, 0.003368499455973506, 0.0035375338047742844, 0.003384218318387866, 0.003355738939717412, 0.0033440380357205868, 0.003611277788877487, 0.003742142114788294, 0.0034063016064465046, 0.0035288899671286345, 0.003418193431571126, 0.0034803547896444798, 0.003499598242342472, 0.003314452711492777, 0.003467183094471693, 0.003495246870443225, 0.0034001939930021763, 0.0034913518466055393, 0.0036317696794867516, 0.003391080070286989, 0.0035447352565824986, 0.0034976848401129246, 0.003405048046261072, 0.003401502501219511, 0.0035025363322347403, 0.0034897527657449245, 0.0033252029679715633, 0.00334293395280838, 0.0033347641583532095, 0.003192863194271922, 0.0034399398136883974, 0.0035879877395927906, 0.0034977830946445465, 0.003365274053066969, 0.00346761173568666, 0.003491081530228257, 0.003484340850263834, 0.003339300397783518, 0.0034125573001801968, 0.0034689423628151417, 0.003271036548539996, 0.0034101922065019608, 0.0034039702732115984, 0.003510585054755211, 0.003394368337467313, 0.003281283425167203, 0.003544153179973364, 0.0035091296304017305, 0.003371148370206356, 0.003547118278220296, 0.0037872176617383957, 0.003398297121748328, 0.003376152366399765, 0.0034977884497493505, 0.0035442423541098833, 0.003542535472661257, 0.0034925283398479223, 0.0035246219485998154, 0.0033332540187984705, 0.0034616689663380384, 0.0036464149598032236, 0.0035818694159388542, 0.003622915130108595, 0.0034888996742665768, 0.003478448837995529, 0.00357696320861578, 0.003355607623234391, 0.003462140914052725, 0.003397169290110469, 0.0034132946748286486, 0.0036189802922308445, 0.0033223237842321396, 0.0034382371231913567, 0.0037126371171325445, 0.0035526917781680822, 0.0034903502091765404, 0.0035289525985717773, 0.003307485021650791, 0.0034409957006573677, 0.003425255883485079, 0.0034229280427098274, 0.0036852823104709387, 0.003580833086743951, 0.0035229611676186323, 0.0035764328204095364, 0.003609841223806143, 0.0034151796717196703, 0.0036085080355405807, 0.0035359677858650684, 0.0035078166984021664, 0.003420487744733691, 0.0036226671654731035, 0.0035954411141574383, 0.003630871418863535, 0.003754772711545229, 0.003395577659830451, 0.0035341335460543633, 0.003553970716893673, 0.003593470435589552, 0.0035139392130076885, 0.0035151513293385506, 0.0033814599737524986, 0.0033430643379688263, 0.003390195081010461, 0.003532111644744873, 0.0033741502556949854, 0.0034011665266007185, 0.003673949744552374, 0.003450672375038266, 0.003657235763967037, 0.0033796641509979963, 0.0036910357885062695, 0.0033253843430429697, 0.003387411590665579, 0.003433174453675747, 0.003269228618592024, 0.0035697538405656815, 0.003587110433727503, 0.003399725304916501, 0.00343616446480155, 0.0034677807707339525, 0.003525947220623493, 0.0035051105078309774, 0.0034625083208084106, 0.0035225271712988615, 0.0034065365325659513, 0.0036139709409326315, 0.003617080394178629, 0.0033975348342210054, 0.0034191070590168238, 0.003340876894071698, 0.0033633189741522074, 0.0034431214444339275, 0.0035591288469731808, 0.003505915403366089, 0.003482087282463908, 0.0033457214012742043, 0.003368648234754801, 0.0032431494910269976, 0.0033689914271235466, 0.003354612272232771, 0.0034083090722560883, 0.00358713511377573, 0.0034822579473257065, 0.0035619004629552364, 0.003135888371616602, 0.0033935888204723597, 0.003334395820274949, 0.003416245337575674, 0.003493137191981077, 0.0034470215905457735, 0.0033474082592874765, 0.0036708530969917774, 0.0035520175006240606, 0.00334166013635695, 0.0034949914552271366, 0.0033487596083432436, 0.003366691991686821, 0.0033879063557833433, 0.003657261375337839, 0.003459861036390066, 0.0035024709068238735, 0.0034903837367892265, 0.003569986904039979, 0.003692670026794076, 0.00349485338665545, 0.003474959172308445, 0.0033396719954907894, 0.0034137857146561146, 0.003458632156252861, 0.0034784898161888123, 0.003481481224298477, 0.0031869392842054367, 0.0033993381075561047, 0.0036091862712055445, 0.003315136069431901, 0.0037251804023981094, 0.0034894049167633057, 0.0034451880492269993, 0.0033704652450978756, 0.003341926960274577, 0.003686952870339155, 0.0034742257557809353, 0.0036341906525194645, 0.003603956662118435, 0.0035850179847329855, 0.0033459775149822235, 0.003416516585275531, 0.0035062325187027454, 0.0032621347345411777, 0.003376148408278823, 0.003464640351012349, 0.0035168686881661415, 0.0034179023932665586, 0.003319316543638706, 0.0036293636076152325, 0.0033497163094580173, 0.003632264444604516, 0.003580777905881405, 0.0033929881174117327, 0.003271412802860141, 0.0033657022286206484, 0.003639930859208107, 0.0033373376354575157, 0.0035968346055597067, 0.0032377117313444614, 0.0035883367527276278, 0.0032938742078840733, 0.0034981872886419296, 0.003519065910950303, 0.003599038813263178, 0.003517863107845187, 0.0033348691649734974, 0.0036434498615562916, 0.003490612143650651, 0.003486629808321595, 0.0034801424480974674, 0.003336887340992689, 0.003479291684925556, 0.0035480433143675327, 0.003451685653999448, 0.0033212548587471247, 0.003504578722640872, 0.003420245833694935, 0.003639824688434601, 0.003418892854824662, 0.0035269795916974545, 0.0032760663889348507, 0.003436038503423333, 0.0035662611480802298, 0.00358085660263896, 0.0035339072346687317, 0.00348381488583982, 0.0033504008315503597, 0.0033582602627575397, 0.0034221552778035402, 0.003419854212552309, 0.0034562749788165092, 0.003376510227099061, 0.003456849604845047, 0.0035669137723743916, 0.0033056698739528656, 0.0035605321172624826, 0.003660496324300766, 0.0034740082919597626, 0.003399915061891079, 0.003396870568394661, 0.00364092830568552, 0.0033711642026901245, 0.0035193522926419973, 0.0038131927140057087, 0.0033222089987248182, 0.003384978510439396, 0.0034436366986483335, 0.0032776840962469578, 0.003511895425617695, 0.0036343534011393785, 0.003398582572117448, 0.0033204220235347748, 0.0034522495698183775, 0.0037060980685055256, 0.0033377765212208033, 0.0037077367305755615, 0.0032555917277932167, 0.0034558954648673534, 0.0034881075844168663, 0.003469096263870597, 0.0033084049355238676, 0.0036341052036732435, 0.003415252547711134, 0.0032347021624445915, 0.0034382541198283434, 0.0033580476883798838, 0.003488672897219658, 0.0035964236594736576, 0.0034307879395782948, 0.003376796841621399, 0.00349664269015193, 0.0034599758218973875, 0.003520425409078598, 0.0036885407753288746, 0.003511916846036911, 0.0033321480732411146, 0.0034118969924747944, 0.003498783800750971, 0.003491245908662677, 0.003257600124925375, 0.003525125328451395, 0.003547685919329524, 0.0035355782601982355, 0.0034627823624759912, 0.003617064096033573, 0.003488946706056595, 0.003513561561703682, 0.0035431920550763607, 0.003379139117896557, 0.0033940416760742664, 0.0034084562212228775, 0.0033069271594285965, 0.0034517874009907246, 0.0035218889825046062, 0.003458221908658743, 0.0034057169687002897, 0.003388151293620467, 0.003466807771474123, 0.0035467788111418486, 0.0034874326083809137, 0.0035795189905911684, 0.0033742799423635006, 0.0035869572311639786, 0.003555778879672289, 0.0035649852361530066, 0.003414066741243005, 0.0033687823452055454, 0.003449531039223075, 0.003556781681254506, 0.003594178007915616, 0.0035311877727508545, 0.003365444717928767, 0.003402991918846965, 0.0034978913608938456, 0.0032844922970980406, 0.003435010788962245, 0.0032834955491125584, 0.0033316733315587044, 0.003559607081115246, 0.0034100802149623632, 0.00351084116846323, 0.003517214674502611, 0.0033048782497644424, 0.0032625971361994743, 0.0035282853059470654, 0.0033104708418250084, 0.0035294974222779274, 0.003333008149638772, 0.0034999188501387835, 0.0034609665162861347, 0.003654100466519594, 0.0033895475789904594, 0.0034427186474204063, 0.0036239069886505604, 0.0037513733841478825, 0.0034911043476313353, 0.0035623814910650253, 0.003523642662912607, 0.0035235260147601366, 0.003513294504955411, 0.003470717929303646, 0.0035431385040283203, 0.0035980097018182278, 0.0035698236897587776, 0.0034603909589350224, 0.003262863727286458, 0.003615936730057001, 0.003254765411838889, 0.0033985814079642296, 0.003643827745690942, 0.0036193225532770157, 0.0032284127082675695, 0.0032353210262954235, 0.003327633487060666, 0.003433376085013151, 0.0034544849768280983, 0.003590310923755169, 0.0036100014112889767, 0.0036137665156275034, 0.0035444567911326885, 0.0033540702424943447, 0.0034566230606287718, 0.0035058280918747187, 0.003640079637989402, 0.003713266458362341, 0.0033550846856087446, 0.003410735633224249, 0.0035339645110070705, 0.0034645777195692062, 0.0034301471896469593, 0.0034541278146207333, 0.0034885192289948463, 0.003221752354875207, 0.0034901760518550873, 0.0034616682678461075, 0.003650402184575796, 0.0033468781039118767, 0.003634690772742033, 0.003550134599208832, 0.003596062771975994, 0.003379648784175515, 0.003515098709613085, 0.003479232545942068, 0.0034346929751336575, 0.0034795301035046577, 0.0034330158960074186, 0.0037074668798595667, 0.0033230516128242016, 0.003514738753437996, 0.0033273256849497557, 0.003296209266409278, 0.0034913592971861362, 0.0034626368433237076, 0.003458368591964245, 0.0035446095280349255, 0.0034633008763194084, 0.0034373633097857237, 0.0034402094315737486, 0.0035210540518164635, 0.0037879825104027987, 0.0034965472295880318, 0.0036016665399074554, 0.0034631211310625076, 0.00331686157733202, 0.0034726005978882313, 0.003290214342996478, 0.003350974293425679, 0.003478037193417549, 0.0035406064707785845, 0.0033579322043806314, 0.003252585418522358, 0.0034392608795315027, 0.0036090873181819916, 0.0035469101276248693, 0.0035813546273857355, 0.0035319868475198746, 0.0034793075174093246, 0.0034247436560690403, 0.0035337889567017555, 0.0036042409483343363, 0.003382420167326927, 0.0036065957974642515, 0.0035250873770564795, 0.0032280816230922937, 0.003317460184916854, 0.0037618447095155716, 0.0034576989710330963, 0.0036333168391138315, 0.0035539520904421806, 0.0033323271200060844, 0.003603593213483691, 0.003771283430978656, 0.003810102352872491, 0.0035546491853892803, 0.0030565671622753143, 0.003477188292890787, 0.003466555383056402, 0.0036736743059009314, 0.0034352061338722706, 0.0034794483799487352, 0.0036417092196643353, 0.0033997208811342716, 0.0033866390585899353, 0.0033472017385065556, 0.0033711171709001064, 0.0035604273434728384, 0.0035679987631738186, 0.0033743716776371002, 0.0034109458792954683, 0.003510064212605357, 0.003541296813637018, 0.0034406757913529873, 0.0034856924321502447, 0.00358744733966887, 0.0035782025661319494, 0.003384548472240567, 0.0037055034190416336, 0.00333832623437047, 0.003547258209437132, 0.0035780658945441246, 0.003366745775565505, 0.0036068798508495092, 0.0035475504118949175, 0.0037028766237199306, 0.0035926196724176407, 0.0033822376281023026, 0.0032969010062515736, 0.003559722797945142, 0.0032609649933874607, 0.003486811649054289, 0.0034549380652606487, 0.003436408704146743, 0.003542095422744751, 0.0035121405962854624, 0.0034762690775096416, 0.003468625247478485, 0.003373232437297702, 0.0035593798384070396, 0.00343985203653574, 0.003276966977864504, 0.0033926363103091717, 0.003454219549894333, 0.003498825943097472, 0.0033869734033942223, 0.003460448468104005, 0.0034534388687461615, 0.0036060635466128588, 0.0034412643872201443, 0.0035333444830030203, 0.0032852969598025084, 0.0036077378317713737, 0.00338359409943223, 0.0032896825578063726, 0.003551967442035675, 0.0031818337738513947, 0.003473019227385521, 0.0035525085404515266, 0.0034374112728983164, 0.0037529717665165663, 0.00338186277076602, 0.003545447252690792, 0.0036170948296785355, 0.0035132220946252346, 0.0033477316610515118, 0.0032823020592331886, 0.0033556120470166206, 0.0031936815939843655, 0.0035861257929354906, 0.00371563620865345] [0.0033891741186380386, 0.0033919261768460274, 0.0036300162319093943, 0.003409250872209668, 0.003403794951736927, 0.0033538215793669224, 0.003691673744469881, 0.003362557850778103, 0.0034775142557919025, 0.0037210385780781507, 0.003653774270787835, 0.0033154692500829697, 0.003398505039513111, 0.003485451452434063, 0.003479821141809225, 0.003496417310088873, 0.0034323828294873238, 0.0032665522303432226, 0.0033396268263459206, 0.0032858538907021284, 0.0035892734304070473, 0.003685862524434924, 0.0034877017606049776, 0.003684035502374172, 0.0036067229229956865, 0.003514110343530774, 0.0033492576330900192, 0.0035211783833801746, 0.003564015030860901, 0.0033116107806563377, 0.0031845683697611094, 0.0034863599576056004, 0.003493677591904998, 0.003292631823569536, 0.0033584346529096365, 0.0034811764489859343, 0.003602296579629183, 0.003512302413582802, 0.003641944145783782, 0.0033576725982129574, 0.0033075655810534954, 0.0034927884116768837, 0.0034067914821207523, 0.003399206092581153, 0.003462061285972595, 0.0033895757514983416, 0.003300188574939966, 0.003448889357969165, 0.0034155594184994698, 0.003601138712838292, 0.0035509124863892794, 0.0035160877741873264, 0.0036651187110692263, 0.0035643780138343573, 0.0033666533417999744, 0.003557239891961217, 0.0034431787207722664, 0.0035183620639145374, 0.003479467239230871, 0.003305140184238553, 0.003400001209229231, 0.0033114589750766754, 0.00346425361931324, 0.003318788018077612, 0.0036182496696710587, 0.0033385897986590862, 0.0032207490876317024, 0.0036099241115152836, 0.003135251346975565, 0.0034845564514398575, 0.0035202340222895145, 0.003387672360986471, 0.003504120511934161, 0.003491607727482915, 0.0032641207799315453, 0.0036459313705563545, 0.0034406371414661407, 0.0035029365681111813, 0.0035095438361167908, 0.003371323924511671, 0.0032441397197544575, 0.003539766650646925, 0.0033873990178108215, 0.0034943483769893646, 0.0034424085170030594, 0.003432556753978133, 0.0033793202601373196, 0.0034517324529588223, 0.003733446355909109, 0.003448459319770336, 0.003463730914518237, 0.0035619880072772503, 0.0035140698309987783, 0.003410928649827838, 0.0035504258703440428, 0.0036639354657381773, 0.0036721741780638695, 0.0034333555959165096, 0.0034043416380882263, 0.0035301900934427977, 0.003238038159906864, 0.0036281675565987825, 0.0032965373247861862, 0.0037716731894761324, 0.0036666467785835266, 0.003402369562536478, 0.0038314317353069782, 0.0032345533836632967, 0.0034854463301599026, 0.0035372921265661716, 0.0036377727519720793, 0.003311488777399063, 0.0037507652305066586, 0.003541650716215372, 0.0032180179841816425, 0.003379747737199068, 0.0035863500088453293, 0.00375916576012969, 0.0034075286239385605, 0.003471873700618744, 0.0034821894951164722, 0.003627573372796178, 0.0035926220007240772, 0.003459121333435178, 0.003260307013988495, 0.0034465324133634567, 0.003402140224352479, 0.0037230998277664185, 0.00327285286039114, 0.003373844549059868, 0.003439203603193164, 0.003643547184765339, 0.0033162038307636976, 0.0033432496711611748, 0.0035247153136879206, 0.0033702380023896694, 0.0035089317243546247, 0.003629836020991206, 0.003411729820072651, 0.0033935324754565954, 0.003561075311154127, 0.0034821301233023405, 0.003305302234366536, 0.0034101619385182858, 0.003436824306845665, 0.003391219535842538, 0.0034417901188135147, 0.003311648964881897, 0.0033710377756506205, 0.0033400298561900854, 0.0033863126300275326, 0.003565151710063219, 0.003331532469019294, 0.0033599489834159613, 0.0033929524943232536, 0.003598460229113698, 0.0034160988871008158, 0.0037194087635725737, 0.0036218012683093548, 0.0037804439198225737, 0.003279194701462984, 0.003475258592516184, 0.0035571432672441006, 0.0033874991349875927, 0.0034971542190760374, 0.003224796149879694, 0.0035749415401369333, 0.0034249653108417988, 0.0034465626813471317, 0.0034396154806017876, 0.0033218921162188053, 0.003560248762369156, 0.0035600182600319386, 0.0035452465526759624, 0.003505028784275055, 0.003476752433925867, 0.003539745230227709, 0.003344885539263487, 0.003572516143321991, 0.0034243841655552387, 0.0034731472842395306, 0.003438248299062252, 0.0034492393024265766, 0.0033390214666724205, 0.0035029444843530655, 0.003376535838469863, 0.003333987668156624, 0.0034800635185092688, 0.00330538721755147, 0.003453736426308751, 0.003546477761119604, 0.003596089081838727, 0.0034918393939733505, 0.003417941043153405, 0.0032473092433065176, 0.003384692594408989, 0.003295118920505047, 0.003407800104469061, 0.0037148629780858755, 0.00345519557595253, 0.0033854267094284296, 0.00334097258746624, 0.003398669883608818, 0.0034037684090435505, 0.003570019267499447, 0.00350720202550292, 0.00338687002658844, 0.0035640080459415913, 0.0033751854207366705, 0.0034769976045936346, 0.0035491599701344967, 0.003423798829317093, 0.003493410535156727, 0.003572290064767003, 0.003469443880021572, 0.0035569951869547367, 0.0032608427572995424, 0.0036011389456689358, 0.003605837468057871, 0.0035026937257498503, 0.0032805893570184708, 0.003403390757739544, 0.0037879948504269123, 0.0034440134186297655, 0.003544481238350272, 0.0034173373132944107, 0.0035371012054383755, 0.0034896759316325188, 0.0032463981769979, 0.0034619062207639217, 0.0035432016011327505, 0.00345079624094069, 0.0033919457346200943, 0.00358380819670856, 0.003553002141416073, 0.0032529737800359726, 0.003624439239501953, 0.003785391105338931, 0.003522144164890051, 0.0033323648385703564, 0.0035428269766271114, 0.0032659024000167847, 0.0033656738232821226, 0.0033630449324846268, 0.0034630699083209038, 0.0032806009985506535, 0.003495369339361787, 0.0034538840409368277, 0.003582383506000042, 0.003572537563741207, 0.003520600264891982, 0.0033545498736202717, 0.0035497182980179787, 0.003527715802192688, 0.003534079296514392, 0.00324153620749712, 0.003493065480142832, 0.0034219149965792894, 0.0034742713905870914, 0.003781840205192566, 0.003403390059247613, 0.003457943443208933, 0.00330825406126678, 0.0035562864504754543, 0.0036134833935648203, 0.0032606408931314945, 0.003480109153315425, 0.0037344349548220634, 0.0034757901448756456, 0.003507968969643116, 0.003693182487040758, 0.0034004643093794584, 0.003401298075914383, 0.0034885723143815994, 0.003260251134634018, 0.003157875267788768, 0.00371941807679832, 0.0034114227164536715, 0.0036580339074134827, 0.0033669203985482454, 0.0033488415647298098, 0.0035651936195790768, 0.003367762081325054, 0.0033907329197973013, 0.003636722220107913, 0.0034390364307910204, 0.003453620942309499, 0.003651582170277834, 0.0034611227456480265, 0.0034082806669175625, 0.0035124069545418024, 0.0035843863151967525, 0.0034520288463681936, 0.003504935884848237, 0.0034483757335692644, 0.003612812841311097, 0.0034214481711387634, 0.00349200121127069, 0.003283893223851919, 0.0032302290201187134, 0.003466877853497863, 0.0032878888305276632, 0.0034104459919035435, 0.0034852821845561266, 0.0034290270414203405, 0.0032645761966705322, 0.0035909409634768963, 0.0035435480531305075, 0.003537163371220231, 0.003406757488846779, 0.0034227073192596436, 0.0033808143343776464, 0.003510550130158663, 0.003431453835219145, 0.003454268677160144, 0.0035012513399124146, 0.003402490634471178, 0.00353981158696115, 0.0035110912285745144, 0.0034202493261545897, 0.003480209968984127, 0.0036034858785569668, 0.0033411711920052767, 0.0035422667860984802, 0.0035480044316500425, 0.003635925240814686, 0.0035104770213365555, 0.003357495879754424, 0.0034085684455931187, 0.0033236078452318907, 0.003606438869610429, 0.0033350095618516207, 0.0033474580850452185, 0.0036326374392956495, 0.0034811506047844887, 0.0037204367108643055, 0.0034282829146832228, 0.0035295358393341303, 0.003436419414356351, 0.003550597233697772, 0.0034666843712329865, 0.003493677591904998, 0.003416585037484765, 0.0035050425212830305, 0.0034835708793252707, 0.0035394676961004734, 0.0035073342733085155, 0.00367167335934937, 0.0033920714631676674, 0.0032764598727226257, 0.0033478657715022564, 0.0033171221148222685, 0.003644533921033144, 0.003471266943961382, 0.0035477005876600742, 0.003389140823855996, 0.0033484797459095716, 0.003566961968317628, 0.0033405751455575228, 0.003660712856799364, 0.003617672249674797, 0.003397934138774872, 0.0032174037769436836, 0.003562766592949629, 0.0035655596293509007, 0.0031871816609054804, 0.0036031422205269337, 0.003580971620976925, 0.003560259472578764, 0.0031613195315003395, 0.003467295551672578, 0.0034887799993157387, 0.003399391658604145, 0.0032968088053166866, 0.003333547618240118, 0.0033204585779458284, 0.0034476618748158216, 0.0033163269981741905, 0.0034998967312276363, 0.003370109712705016, 0.0035414688754826784, 0.003489152295514941, 0.003588899038732052, 0.003532933769747615, 0.003458706894889474, 0.0033243729267269373, 0.0035750633105635643, 0.003535857889801264, 0.003423333866521716, 0.0034027504734694958, 0.003448914736509323, 0.00335161411203444, 0.003280157456174493, 0.003460567444562912, 0.003343043616041541, 0.0038572894409298897, 0.0035374248400330544, 0.0032407173421233892] [] [] [] []\n"
     ]
    }
   ],
   "source": [
    "print(*loss_list)\n",
    "\n",
    "path = save_dir + 'loss2.txt'\n",
    "os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "with open (path, 'w') as f:\n",
    "    for loss in loss_list:\n",
    "        f.write(' '.join(map(str, loss)) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "with open(\"loss.txt\", \"r\") as f:\n",
    "    loss_list = [list(map(float, line.split())) for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "记录的loss数量: 1838\n",
      "最后一个loss: 0.5641474723815918\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y = [e for l in loss_list for e in l]\n",
    "print(f\"记录的loss数量: {len(y)}\")\n",
    "print(f\"最后一个loss: {y[-1]}\")\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在测试集上评估 Encoder, Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载模型\n",
    "encoder = MLPEncoder(num_in=K, num_out=R, in_dim=split_data_shape)\n",
    "decoder = MLPDecoder(num_in=N, num_out=K, in_dim=split_conv_output_data_shape)\n",
    "\n",
    "encoder.load_state_dict(torch.load(encoder_path))\n",
    "decoder.load_state_dict(torch.load(decoder_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset: 50000\n",
      "Test dataset: 10000\n",
      "image size:  torch.Size([3, 32, 32])\n",
      "Device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test: 100%|████████████████████| 157/157 [00:32<00:00,  4.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "测试集总数: 10000\n",
      "原始模型 -> 预测正确数: 5229, 预测准确率: 52.29%\n",
      "合并模型(conv+fc) -> 预测正确数: 5229, 预测准确率: 52.29%\n",
      "使用Encoder和Decoder -> 预测正确数: 1000, 预测准确率: 10.0%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "from dataset.image_dataset import ImageDataset\n",
    "from util.split_data import split_vector\n",
    "\n",
    "print(f\"Train dataset: {len(train_dataset)}\")\n",
    "print(f\"Test dataset: {len(test_dataset)}\")\n",
    "print(\"image size: \", train_dataset[0][0].size())\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "\n",
    "conv_segment.to(device)\n",
    "fc_segment.to(device)\n",
    "model.to(device)\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n",
    "\n",
    "conv_segment.eval()\n",
    "fc_segment.eval()\n",
    "model.eval()\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "\n",
    "original_correct = 0\n",
    "merge_correct = 0\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    test_loader_tqdm = tqdm(\n",
    "        test_loader,\n",
    "        desc=f\"Test\",\n",
    "        bar_format=\"{l_bar}{bar:20}{r_bar}\",\n",
    "    )\n",
    "    for images, labels in test_loader_tqdm:\n",
    "\n",
    "        # split image tensor(64, 3, 32, 32) -> [tensor(64, 3, 32, 8) * K]\n",
    "        images_list = []\n",
    "        for start, end in split_vector(L=original_data_shape[2], k=K, l=split_data_shape[2]):\n",
    "            images_list.append(images[:,:,:,start:end].clone())\n",
    "\n",
    "        images_list = [images.to(device) for images in images_list]\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        images = images.to(device)\n",
    "        ground_truth = conv_segment(images)\n",
    "\n",
    "        _, predicted = torch.max(model(images).data, 1)\n",
    "        original_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        output = conv_segment(images)\n",
    "        output = output.view(output.size(0), -1)\n",
    "        output = fc_segment(output)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        merge_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        imageDataset_list = [\n",
    "            ImageDataset(images)\n",
    "            for images in images_list + encoder(images_list)\n",
    "        ]\n",
    "        output_list = []\n",
    "        for i in range(N):\n",
    "            imageDataset = imageDataset_list[i]\n",
    "            output = conv_segment(imageDataset.images)\n",
    "            output_list.append(output)\n",
    "        # losed_output_list = lose_something(output_list, self.lose_device_index)\n",
    "        decoded_output_list = decoder(output_list)\n",
    "        output = torch.cat(decoded_output_list, dim=3)\n",
    "        output = output.view(output.size(0), -1)\n",
    "\n",
    "        _, predicted = torch.max(fc_segment(output).data, 1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "print(f\"测试集总数: {total}\")\n",
    "print(f\"原始模型 -> 预测正确数: {original_correct}, 预测准确率: {100 * original_correct / total}%\")\n",
    "print(f\"合并模型(conv+fc) -> 预测正确数: {merge_correct}, 预测准确率: {100 * merge_correct / total}%\")\n",
    "print(f\"使用Encoder和Decoder -> 预测正确数: {correct}, 预测准确率: {100 * correct / total}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'device' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mf:\\MyCourse(5 delayed 1)\\erasure code\\non-linear erasure code\\src\\NewMethod\\test1.ipynb 单元格 13\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/f%3A/MyCourse%285%20delayed%201%29/erasure%20code/non-linear%20erasure%20code/src/NewMethod/test1.ipynb#X15sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mrandn(\u001b[39m1\u001b[39m, \u001b[39m*\u001b[39moriginal_data_shape)\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/MyCourse%285%20delayed%201%29/erasure%20code/non-linear%20erasure%20code/src/NewMethod/test1.ipynb#X15sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mx 经过完整的 model 得: \u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/f%3A/MyCourse%285%20delayed%201%29/erasure%20code/non-linear%20erasure%20code/src/NewMethod/test1.ipynb#X15sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mprint\u001b[39m(model(x)\u001b[39m.\u001b[39mdata)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'device' is not defined"
     ]
    }
   ],
   "source": [
    "x = torch.randn(1, *original_data_shape).to(device)\n",
    "\n",
    "print(\"x 经过完整的 model 得: \")\n",
    "print(model(x).data)\n",
    "\n",
    "y = conv_segment(x)\n",
    "y = y.view(y.size(0), -1)\n",
    "output_truth = y.data\n",
    "y = fc_segment(y)\n",
    "print(\"x 经过 conv 和 fc 得: \")\n",
    "print(y.data)\n",
    "\n",
    "print(\"-\" * 20)\n",
    "\n",
    "model.eval()\n",
    "for m in model.modules():\n",
    "    if isinstance(m, nn.BatchNorm2d):\n",
    "        m.track_running_stats = False\n",
    "\n",
    "images_list = []\n",
    "for start, end in split_vector(L=original_data_shape[2], k=2, l=16):\n",
    "    print(start, end)\n",
    "    images_list.append(x[:, :, :, start:end].clone())\n",
    "\n",
    "images_list = [images.to(device) for images in images_list]\n",
    "\n",
    "output_list = []\n",
    "for i in range(len(images_list)):\n",
    "    output = conv_segment(images_list[i])\n",
    "    output_list.append(output)\n",
    "    print(output.shape)\n",
    "output = torch.cat(output_list, dim=3)  # changed\n",
    "output = output.view(output.size(0), -1).data\n",
    "print(output.shape, output_truth.shape)\n",
    "print(output)\n",
    "print(output_truth)\n",
    "\n",
    "print(model)\n",
    "print(\"-\" * 20)\n",
    "import torch.nn as nn\n",
    "layers = nn.Sequential(\n",
    "    nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "    nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "    nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "    nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1, bias=False),\n",
    "    nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1, bias=False),\n",
    ").to(device)\n",
    "x1 = x\n",
    "x2 = x[:, :, :, 0:16]\n",
    "for layer in layers:\n",
    "    record_layer = layer\n",
    "    record_x1 = x1.clone()\n",
    "    record_x2 = x2.clone()\n",
    "    x1 = layer(x1)\n",
    "    x2 = layer(x2)\n",
    "    print(\"模型\")\n",
    "    print(layer)\n",
    "    print(\"输出大小\")\n",
    "    print(x1.shape, x2.shape)\n",
    "    print(\"0,0,0,0 值:\")\n",
    "    print(x1[0, 0, 0, 0].data, x2[0, 0, 0, 0].data)\n",
    "    print(\"0,0,0,1 值:\")\n",
    "    print(x1[0, 0, 0, 1].data, x2[0, 0, 0, 1].data)\n",
    "    print(\"是否相等:\")\n",
    "    # print(x1[:, :, :, 0 : x2.shape[3]].data)\n",
    "    # print(x2.data)\n",
    "    is_equal = torch.all(torch.eq(x1[:, :, :, 0 : x2.shape[3]].data, x2.data))\n",
    "    print(is_equal)\n",
    "    print(x1[0, 0, 0, 0 : x2.shape[3]].data == x2[0, 0, 0, :].data)\n",
    "    print(\"-\" * 20)\n",
    "    print(\"\\n\\n\\n\")\n",
    "\n",
    "\n",
    "# `print('-'*20)\n",
    "\n",
    "# import torch.nn as nn\n",
    "# layer = nn.BatchNorm2d(3)\n",
    "# # layer = nn.Conv2d(3, 64, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "# # layer = nn.ReLU(inplace=True)\n",
    "# layer.to(device)\n",
    "# layer.eval()\n",
    "# layer.track_running_stats = False\n",
    "# output_truth = layer(x)\n",
    "# output_truth = output_truth.view(output_truth.size(0), -1).data\n",
    "\n",
    "# output_list = []\n",
    "# for i in range(N):\n",
    "#     output = layer(images_list[i])\n",
    "#     output_list.append(output)\n",
    "# output = torch.cat(output_list, dim=3)  # changed\n",
    "# output = output.view(output.size(0), -1).data\n",
    "# print(output.shape, output_truth.shape)\n",
    "# print(output)\n",
    "# print(output_truth)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练 base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset: 50000\n",
      "Test dataset: 10000\n",
      "image size:  torch.Size([3, 32, 32])\n",
      "Device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 782/782 [01:28<00:00,  8.86it/s, loss=1.79] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 1.787920594215393\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 782/782 [01:27<00:00,  8.92it/s, loss=1.36] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Loss: 1.3618195056915283\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 782/782 [01:25<00:00,  9.16it/s, loss=0.804]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Loss: 0.8042820692062378\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 782/782 [01:25<00:00,  9.10it/s, loss=0.738]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Loss: 0.7381492853164673\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 782/782 [01:24<00:00,  9.26it/s, loss=0.719]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Loss: 0.7186200022697449\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 782/782 [01:26<00:00,  9.06it/s, loss=0.364]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Loss: 0.3641059696674347\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 782/782 [01:25<00:00,  9.13it/s, loss=0.578] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Loss: 0.5779234766960144\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 782/782 [01:25<00:00,  9.14it/s, loss=0.0786]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Loss: 0.07857557386159897\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 782/782 [01:26<00:00,  9.03it/s, loss=0.206]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Loss: 0.20561690628528595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 782/782 [01:25<00:00,  9.17it/s, loss=0.0971] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Loss: 0.09707945585250854\n",
      "Accuracy on the Test set: 66.21%\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "\n",
    "print(f\"Train dataset: {len(train_dataset)}\")\n",
    "print(f\"Test dataset: {len(test_dataset)}\")\n",
    "print(\"image size: \", train_dataset[0][0].size())\n",
    "\n",
    "# 部署到设备\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "model.to(device)\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0)\n",
    "\n",
    "t0 = datetime.datetime.now()\n",
    "# 训练循环\n",
    "model.train()  # 设置模型为训练模式\n",
    "num_epochs = 10  # 迭代次数\n",
    "for epoch in range(num_epochs):\n",
    "    # 使用 tqdm 包装训练数据加载器\n",
    "    train_loader_tqdm = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    for data, target in train_loader_tqdm:\n",
    "        # 将数据移动到设备上\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        # target 变为 one-hot 编码\n",
    "        target = (\n",
    "            torch.zeros(target.size(0), 10)\n",
    "            .to(device)\n",
    "            .scatter_(1, target.view(-1, 1), 1)\n",
    "        )\n",
    "\n",
    "        # 正向传播\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # 反向传播和优化\n",
    "        optimizer.zero_grad()  # 清除之前的梯度\n",
    "        loss.backward()  # 反向传播\n",
    "        optimizer.step()  # 更新权重\n",
    "\n",
    "        # 更新进度条的描述\n",
    "        train_loader_tqdm.set_postfix(loss=loss.item())\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}\")\n",
    "t1 = datetime.datetime.now()\n",
    "\n",
    "# 测试循环\n",
    "model.eval()  # 设置模型为评估模式\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():  # 在评估过程中不计算梯度\n",
    "    for data, target in train_loader:\n",
    "        # 将数据移动到设备上\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        output = model(data)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        total += target.size(0)\n",
    "        correct += (predicted == target).sum().item()\n",
    "\n",
    "print(f\"训练集-> 总量: {total}, 正确数量: {correct}, 准确率: {100 * correct / total}%\")\n",
    "t2 = datetime.datetime.now()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():  # 在评估过程中不计算梯度\n",
    "    for data, target in test_loader:\n",
    "        # 将数据移动到设备上\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        output = model(data)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        total += target.size(0)\n",
    "        correct += (predicted == target).sum().item()\n",
    "\n",
    "print(f\"测试集-> 总量: {total}, 正确数量: {correct}, 准确率: {100 * correct / total}%\")\n",
    "t3 = datetime.datetime.now()\n",
    "\n",
    "print(f\"训练时间: {t1 - t0}\")\n",
    "print(f\"训练集评估时间: {t2 - t1}\")\n",
    "print(f\"测试集评估时间: {t3 - t2}\")\n",
    "\n",
    "# 保存模型\n",
    "now = datetime.datetime.now()\n",
    "date = now.strftime(\"%Y_%m_%d\")\n",
    "filepath = f\"base_model/{TASK_CONFIG['MODEL']}/{TASK_CONFIG['TASK']}/{TASK_CONFIG['DATE']}/model.pth\"\n",
    "dirpath = os.path.dirname(filepath)\n",
    "if not os.path.exists(dirpath):\n",
    "    os.makedirs(dirpath)\n",
    "torch.save(model.state_dict(), filepath)\n",
    "\n",
    "# 读取模型\n",
    "# model = LeNet5(input_dim=(1, 28, 28), num_classes=10)\n",
    "# model.load_state_dict(torch.load(filepath))\n",
    "\n",
    "# gpu: 3min58s\n",
    "# cpu: 5min17s"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ming",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

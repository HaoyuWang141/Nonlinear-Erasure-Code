{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "original dir:  e:\\Nonlinear-Erasure-Code\\src\\NewMethod\n",
      "changed dir:  e:\\Nonlinear-Erasure-Code\\src\n"
     ]
    }
   ],
   "source": [
    "# You only need to run this section once.\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import os\n",
    "\n",
    "print(\"original dir: \", os.getcwd())\n",
    "\n",
    "if os.getcwd().endswith(\"NewMethod\"):\n",
    "    new_path = \"../\"\n",
    "    os.chdir(new_path)\n",
    "    print(\"changed dir: \", os.getcwd())\n",
    "    \n",
    "import torch\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "TASK_CONFIG = {\n",
    "    \"TASK\": \"MNIST\",\n",
    "    \"DATE\": datetime.datetime.now().strftime(\"%Y_%m_%d\"),\n",
    "    \"MODEL\": \"MyModel1\",\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读取数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9912422/9912422 [00:00<00:00, 10592724.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 28881/28881 [00:00<00:00, 508902.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1648877/1648877 [00:00<00:00, 3790476.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 4542/4542 [00:00<00:00, 4350429.04it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "\n",
      "Data is ready!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 设置数据转换\n",
    "transform = transforms.Compose(\n",
    "    [transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))]\n",
    ")\n",
    "\n",
    "# 设置数据集（训练集与测试集合）\n",
    "\n",
    "\"\"\"\n",
    "MNIST:\n",
    "image: (1, 28, 28), class: 10\n",
    "\"\"\"\n",
    "\n",
    "# MNIST\n",
    "dataset_name = TASK_CONFIG[\"TASK\"]\n",
    "train_dataset = datasets.MNIST(\n",
    "    root=\"./data\", train=True, download=True, transform=transform\n",
    ")\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_dataset = datasets.MNIST(\n",
    "    root=\"./data\", train=False, download=True, transform=transform\n",
    ")\n",
    "test_loader = DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
    "\n",
    "print(\"Data is ready!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "设置部分参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "K: 4\n",
      "R: 0\n",
      "N: 4\n",
      "data_shape: (1, 28, 28)\n",
      "num_classes: 10\n",
      "base_model_path: ./base_model/MyModel1/MNIST/2024_03_05/model.pth\n",
      "encoder_path: ./encoder/MLP/MNIST/2024_03_05/encoder-task_MNIST-basemodel_MyModel1-in_4-out_0.pth\n",
      "decoder_path: ./decoder/MLP/MNIST/2024_03_05/decoder-task_MNIST-basemodel_MyModel1-in_4-out_4.pth\n",
      "save_dir: ./save/MNIST/MyModel1/2024_03_05/\n",
      "epoch_num: 4\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "\n",
    "K = 4\n",
    "R = 0\n",
    "N = K + R\n",
    "original_data_shape = tuple(train_dataset[0][0].shape)\n",
    "num_classes = 10\n",
    "print(f\"K: {K}\")\n",
    "print(f\"R: {R}\")\n",
    "print(f\"N: {N}\")\n",
    "print(f\"data_shape: {original_data_shape}\")\n",
    "print(f\"num_classes: {num_classes}\")\n",
    "\n",
    "base_model_path = (\n",
    "    f\"./base_model/{TASK_CONFIG['MODEL']}/{TASK_CONFIG['TASK']}/2024_03_05/model.pth\"\n",
    ")\n",
    "encoder_path = f\"./encoder/MLP/{TASK_CONFIG['TASK']}/{TASK_CONFIG['DATE']}/encoder-task_{TASK_CONFIG['TASK']}-basemodel_{TASK_CONFIG['MODEL']}-in_{K}-out_{R}.pth\"\n",
    "decoder_path = f\"./decoder/MLP/{TASK_CONFIG['TASK']}/{TASK_CONFIG['DATE']}/decoder-task_{TASK_CONFIG['TASK']}-basemodel_{TASK_CONFIG['MODEL']}-in_{N}-out_{K}.pth\"\n",
    "save_dir = f\"./save/{TASK_CONFIG['TASK']}/{TASK_CONFIG['MODEL']}/{TASK_CONFIG['DATE']}/\"\n",
    "print(f\"base_model_path: {base_model_path}\")\n",
    "print(f\"encoder_path: {encoder_path}\")\n",
    "print(f\"decoder_path: {decoder_path}\")\n",
    "print(f\"save_dir: {save_dir}\")\n",
    "\n",
    "epoch_num = 4\n",
    "print(f\"epoch_num: {epoch_num}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义 base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.4511,  2.8249,  4.0434, -2.8124,  2.7863, -0.4604,  0.9184,  1.5557,\n",
      "          3.1938, -2.1021]])\n",
      "tensor([[ 0.4511,  2.8249,  4.0434, -2.8124,  2.7863, -0.4604,  0.9184,  1.5557,\n",
      "          3.1938, -2.1021]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "from base_model.MyModel1 import MyModel1\n",
    "\n",
    "# 引入 base model, 该model将在后续全部过程中使用\n",
    "# ResNet\n",
    "assert TASK_CONFIG[\"MODEL\"] == \"MyModel1\"\n",
    "model = MyModel1(input_dim=original_data_shape, num_classes=num_classes)\n",
    "\n",
    "# 读取模型\n",
    "model.load_state_dict(torch.load(base_model_path, map_location=device))\n",
    "\n",
    "conv_segment = model.get_conv_segment()\n",
    "fc_segment = model.get_fc_segment()\n",
    "\n",
    "x = torch.randn(1, *original_data_shape)\n",
    "\n",
    "print(model(x).data)\n",
    "\n",
    "y = conv_segment(x)\n",
    "y = y.view(y.size(0), -1)\n",
    "y = fc_segment(y)\n",
    "print(y.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "设置另一部分参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv_output_shape: (64, 20, 20)\n",
      "split_conv_output_shape: (64, 20, 5)\n",
      "{'type': 'ReLU', 'layer': ReLU(inplace=True), 'input_shape': (64, 20, 20), 'output_shape': (64, 20, 20)}\n",
      "(64, 20, 0, 5)\n",
      "(64, 20, 0, 5)\n",
      "--------------------------------------------------\n",
      "{'type': 'BatchNorm2d', 'layer': BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False), 'input_shape': (64, 20, 20), 'output_shape': (64, 20, 20)}\n",
      "(64, 20, 0, 5)\n",
      "(64, 20, 0, 5)\n",
      "--------------------------------------------------\n",
      "{'type': 'Conv2d', 'layer': Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1)), 'input_shape': (32, 22, 22), 'output_shape': (64, 20, 20)}\n",
      "(64, 20, 0, 5)\n",
      "(32, 22, 0, 7)\n",
      "--------------------------------------------------\n",
      "{'type': 'ReLU', 'layer': ReLU(inplace=True), 'input_shape': (32, 22, 22), 'output_shape': (32, 22, 22)}\n",
      "(32, 22, 0, 7)\n",
      "(32, 22, 0, 7)\n",
      "--------------------------------------------------\n",
      "{'type': 'BatchNorm2d', 'layer': BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False), 'input_shape': (32, 22, 22), 'output_shape': (32, 22, 22)}\n",
      "(32, 22, 0, 7)\n",
      "(32, 22, 0, 7)\n",
      "--------------------------------------------------\n",
      "{'type': 'Conv2d', 'layer': Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1)), 'input_shape': (16, 24, 24), 'output_shape': (32, 22, 22)}\n",
      "(32, 22, 0, 7)\n",
      "(16, 24, 0, 9)\n",
      "--------------------------------------------------\n",
      "{'type': 'ReLU', 'layer': ReLU(inplace=True), 'input_shape': (16, 24, 24), 'output_shape': (16, 24, 24)}\n",
      "(16, 24, 0, 9)\n",
      "(16, 24, 0, 9)\n",
      "--------------------------------------------------\n",
      "{'type': 'BatchNorm2d', 'layer': BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False), 'input_shape': (16, 24, 24), 'output_shape': (16, 24, 24)}\n",
      "(16, 24, 0, 9)\n",
      "(16, 24, 0, 9)\n",
      "--------------------------------------------------\n",
      "{'type': 'Conv2d', 'layer': Conv2d(10, 16, kernel_size=(3, 3), stride=(1, 1)), 'input_shape': (10, 26, 26), 'output_shape': (16, 24, 24)}\n",
      "(16, 24, 0, 9)\n",
      "(10, 26, 0, 11)\n",
      "--------------------------------------------------\n",
      "{'type': 'ReLU', 'layer': ReLU(inplace=True), 'input_shape': (10, 26, 26), 'output_shape': (10, 26, 26)}\n",
      "(10, 26, 0, 11)\n",
      "(10, 26, 0, 11)\n",
      "--------------------------------------------------\n",
      "{'type': 'BatchNorm2d', 'layer': BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False), 'input_shape': (10, 26, 26), 'output_shape': (10, 26, 26)}\n",
      "(10, 26, 0, 11)\n",
      "(10, 26, 0, 11)\n",
      "--------------------------------------------------\n",
      "{'type': 'Conv2d', 'layer': Conv2d(1, 10, kernel_size=(3, 3), stride=(1, 1)), 'input_shape': (1, 28, 28), 'output_shape': (10, 26, 26)}\n",
      "(10, 26, 0, 11)\n",
      "(1, 28, 0, 13)\n",
      "--------------------------------------------------\n",
      "{'type': 'ReLU', 'layer': ReLU(inplace=True), 'input_shape': (64, 20, 20), 'output_shape': (64, 20, 20)}\n",
      "(64, 20, 5, 10)\n",
      "(64, 20, 5, 10)\n",
      "--------------------------------------------------\n",
      "{'type': 'BatchNorm2d', 'layer': BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False), 'input_shape': (64, 20, 20), 'output_shape': (64, 20, 20)}\n",
      "(64, 20, 5, 10)\n",
      "(64, 20, 5, 10)\n",
      "--------------------------------------------------\n",
      "{'type': 'Conv2d', 'layer': Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1)), 'input_shape': (32, 22, 22), 'output_shape': (64, 20, 20)}\n",
      "(64, 20, 5, 10)\n",
      "(32, 22, 5, 12)\n",
      "--------------------------------------------------\n",
      "{'type': 'ReLU', 'layer': ReLU(inplace=True), 'input_shape': (32, 22, 22), 'output_shape': (32, 22, 22)}\n",
      "(32, 22, 5, 12)\n",
      "(32, 22, 5, 12)\n",
      "--------------------------------------------------\n",
      "{'type': 'BatchNorm2d', 'layer': BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False), 'input_shape': (32, 22, 22), 'output_shape': (32, 22, 22)}\n",
      "(32, 22, 5, 12)\n",
      "(32, 22, 5, 12)\n",
      "--------------------------------------------------\n",
      "{'type': 'Conv2d', 'layer': Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1)), 'input_shape': (16, 24, 24), 'output_shape': (32, 22, 22)}\n",
      "(32, 22, 5, 12)\n",
      "(16, 24, 5, 14)\n",
      "--------------------------------------------------\n",
      "{'type': 'ReLU', 'layer': ReLU(inplace=True), 'input_shape': (16, 24, 24), 'output_shape': (16, 24, 24)}\n",
      "(16, 24, 5, 14)\n",
      "(16, 24, 5, 14)\n",
      "--------------------------------------------------\n",
      "{'type': 'BatchNorm2d', 'layer': BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False), 'input_shape': (16, 24, 24), 'output_shape': (16, 24, 24)}\n",
      "(16, 24, 5, 14)\n",
      "(16, 24, 5, 14)\n",
      "--------------------------------------------------\n",
      "{'type': 'Conv2d', 'layer': Conv2d(10, 16, kernel_size=(3, 3), stride=(1, 1)), 'input_shape': (10, 26, 26), 'output_shape': (16, 24, 24)}\n",
      "(16, 24, 5, 14)\n",
      "(10, 26, 5, 16)\n",
      "--------------------------------------------------\n",
      "{'type': 'ReLU', 'layer': ReLU(inplace=True), 'input_shape': (10, 26, 26), 'output_shape': (10, 26, 26)}\n",
      "(10, 26, 5, 16)\n",
      "(10, 26, 5, 16)\n",
      "--------------------------------------------------\n",
      "{'type': 'BatchNorm2d', 'layer': BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False), 'input_shape': (10, 26, 26), 'output_shape': (10, 26, 26)}\n",
      "(10, 26, 5, 16)\n",
      "(10, 26, 5, 16)\n",
      "--------------------------------------------------\n",
      "{'type': 'Conv2d', 'layer': Conv2d(1, 10, kernel_size=(3, 3), stride=(1, 1)), 'input_shape': (1, 28, 28), 'output_shape': (10, 26, 26)}\n",
      "(10, 26, 5, 16)\n",
      "(1, 28, 5, 18)\n",
      "--------------------------------------------------\n",
      "{'type': 'ReLU', 'layer': ReLU(inplace=True), 'input_shape': (64, 20, 20), 'output_shape': (64, 20, 20)}\n",
      "(64, 20, 10, 15)\n",
      "(64, 20, 10, 15)\n",
      "--------------------------------------------------\n",
      "{'type': 'BatchNorm2d', 'layer': BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False), 'input_shape': (64, 20, 20), 'output_shape': (64, 20, 20)}\n",
      "(64, 20, 10, 15)\n",
      "(64, 20, 10, 15)\n",
      "--------------------------------------------------\n",
      "{'type': 'Conv2d', 'layer': Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1)), 'input_shape': (32, 22, 22), 'output_shape': (64, 20, 20)}\n",
      "(64, 20, 10, 15)\n",
      "(32, 22, 10, 17)\n",
      "--------------------------------------------------\n",
      "{'type': 'ReLU', 'layer': ReLU(inplace=True), 'input_shape': (32, 22, 22), 'output_shape': (32, 22, 22)}\n",
      "(32, 22, 10, 17)\n",
      "(32, 22, 10, 17)\n",
      "--------------------------------------------------\n",
      "{'type': 'BatchNorm2d', 'layer': BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False), 'input_shape': (32, 22, 22), 'output_shape': (32, 22, 22)}\n",
      "(32, 22, 10, 17)\n",
      "(32, 22, 10, 17)\n",
      "--------------------------------------------------\n",
      "{'type': 'Conv2d', 'layer': Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1)), 'input_shape': (16, 24, 24), 'output_shape': (32, 22, 22)}\n",
      "(32, 22, 10, 17)\n",
      "(16, 24, 10, 19)\n",
      "--------------------------------------------------\n",
      "{'type': 'ReLU', 'layer': ReLU(inplace=True), 'input_shape': (16, 24, 24), 'output_shape': (16, 24, 24)}\n",
      "(16, 24, 10, 19)\n",
      "(16, 24, 10, 19)\n",
      "--------------------------------------------------\n",
      "{'type': 'BatchNorm2d', 'layer': BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False), 'input_shape': (16, 24, 24), 'output_shape': (16, 24, 24)}\n",
      "(16, 24, 10, 19)\n",
      "(16, 24, 10, 19)\n",
      "--------------------------------------------------\n",
      "{'type': 'Conv2d', 'layer': Conv2d(10, 16, kernel_size=(3, 3), stride=(1, 1)), 'input_shape': (10, 26, 26), 'output_shape': (16, 24, 24)}\n",
      "(16, 24, 10, 19)\n",
      "(10, 26, 10, 21)\n",
      "--------------------------------------------------\n",
      "{'type': 'ReLU', 'layer': ReLU(inplace=True), 'input_shape': (10, 26, 26), 'output_shape': (10, 26, 26)}\n",
      "(10, 26, 10, 21)\n",
      "(10, 26, 10, 21)\n",
      "--------------------------------------------------\n",
      "{'type': 'BatchNorm2d', 'layer': BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False), 'input_shape': (10, 26, 26), 'output_shape': (10, 26, 26)}\n",
      "(10, 26, 10, 21)\n",
      "(10, 26, 10, 21)\n",
      "--------------------------------------------------\n",
      "{'type': 'Conv2d', 'layer': Conv2d(1, 10, kernel_size=(3, 3), stride=(1, 1)), 'input_shape': (1, 28, 28), 'output_shape': (10, 26, 26)}\n",
      "(10, 26, 10, 21)\n",
      "(1, 28, 10, 23)\n",
      "--------------------------------------------------\n",
      "{'type': 'ReLU', 'layer': ReLU(inplace=True), 'input_shape': (64, 20, 20), 'output_shape': (64, 20, 20)}\n",
      "(64, 20, 15, 20)\n",
      "(64, 20, 15, 20)\n",
      "--------------------------------------------------\n",
      "{'type': 'BatchNorm2d', 'layer': BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False), 'input_shape': (64, 20, 20), 'output_shape': (64, 20, 20)}\n",
      "(64, 20, 15, 20)\n",
      "(64, 20, 15, 20)\n",
      "--------------------------------------------------\n",
      "{'type': 'Conv2d', 'layer': Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1)), 'input_shape': (32, 22, 22), 'output_shape': (64, 20, 20)}\n",
      "(64, 20, 15, 20)\n",
      "(32, 22, 15, 22)\n",
      "--------------------------------------------------\n",
      "{'type': 'ReLU', 'layer': ReLU(inplace=True), 'input_shape': (32, 22, 22), 'output_shape': (32, 22, 22)}\n",
      "(32, 22, 15, 22)\n",
      "(32, 22, 15, 22)\n",
      "--------------------------------------------------\n",
      "{'type': 'BatchNorm2d', 'layer': BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False), 'input_shape': (32, 22, 22), 'output_shape': (32, 22, 22)}\n",
      "(32, 22, 15, 22)\n",
      "(32, 22, 15, 22)\n",
      "--------------------------------------------------\n",
      "{'type': 'Conv2d', 'layer': Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1)), 'input_shape': (16, 24, 24), 'output_shape': (32, 22, 22)}\n",
      "(32, 22, 15, 22)\n",
      "(16, 24, 15, 24)\n",
      "--------------------------------------------------\n",
      "{'type': 'ReLU', 'layer': ReLU(inplace=True), 'input_shape': (16, 24, 24), 'output_shape': (16, 24, 24)}\n",
      "(16, 24, 15, 24)\n",
      "(16, 24, 15, 24)\n",
      "--------------------------------------------------\n",
      "{'type': 'BatchNorm2d', 'layer': BatchNorm2d(16, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False), 'input_shape': (16, 24, 24), 'output_shape': (16, 24, 24)}\n",
      "(16, 24, 15, 24)\n",
      "(16, 24, 15, 24)\n",
      "--------------------------------------------------\n",
      "{'type': 'Conv2d', 'layer': Conv2d(10, 16, kernel_size=(3, 3), stride=(1, 1)), 'input_shape': (10, 26, 26), 'output_shape': (16, 24, 24)}\n",
      "(16, 24, 15, 24)\n",
      "(10, 26, 15, 26)\n",
      "--------------------------------------------------\n",
      "{'type': 'ReLU', 'layer': ReLU(inplace=True), 'input_shape': (10, 26, 26), 'output_shape': (10, 26, 26)}\n",
      "(10, 26, 15, 26)\n",
      "(10, 26, 15, 26)\n",
      "--------------------------------------------------\n",
      "{'type': 'BatchNorm2d', 'layer': BatchNorm2d(10, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False), 'input_shape': (10, 26, 26), 'output_shape': (10, 26, 26)}\n",
      "(10, 26, 15, 26)\n",
      "(10, 26, 15, 26)\n",
      "--------------------------------------------------\n",
      "{'type': 'Conv2d', 'layer': Conv2d(1, 10, kernel_size=(3, 3), stride=(1, 1)), 'input_shape': (1, 28, 28), 'output_shape': (10, 26, 26)}\n",
      "(10, 26, 15, 26)\n",
      "(1, 28, 15, 28)\n",
      "--------------------------------------------------\n",
      "split_data_range: [(1, 28, 0, 13), (1, 28, 5, 18), (1, 28, 10, 23), (1, 28, 15, 28)]\n",
      "split_conv_output_data_shape from split_data_shape: [(1, 64, 20, 5), (1, 64, 20, 5), (1, 64, 20, 5), (1, 64, 20, 5)]\n"
     ]
    }
   ],
   "source": [
    "from util.util import cal_input_shape\n",
    "\n",
    "\n",
    "conv_output_shape = model.calculate_conv_output(input_dim=original_data_shape)\n",
    "print(f\"conv_output_shape: {conv_output_shape}\")\n",
    "assert conv_output_shape[2] % K == 0\n",
    "\n",
    "split_conv_output_shape = (\n",
    "    conv_output_shape[0],\n",
    "    conv_output_shape[1],\n",
    "    conv_output_shape[2] // K,\n",
    ")\n",
    "print(f\"split_conv_output_shape: {split_conv_output_shape}\")\n",
    "\n",
    "split_data_range = cal_input_shape(\n",
    "    model=conv_segment,\n",
    "    original_input_shape=original_data_shape,\n",
    "    original_output_shape=conv_output_shape,\n",
    "    split_num=K,\n",
    ")\n",
    "print(f\"split_data_range: {split_data_range}\")\n",
    "\n",
    "# print(conv_segment)\n",
    "print(\n",
    "    f\"split_conv_output_data_shape from split_data_shape: {[tuple(conv_segment(torch.randn(1, _[0], _[1], _[3] - _[2])).shape) for _ in split_data_range]}\"\n",
    ")\n",
    "\n",
    "split_data_shape = split_data_range[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "定义 Encoder Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from encoder.mlp_encoder import MLPEncoder\n",
    "from encoder.conv_encoder import CatChannelConvEncoder, CatBatchSizeConvEncoder\n",
    "from decoder.mlp_decoder import MLPDecoder\n",
    "from decoder.conv_decoder import CatChannelConvDecoder, CatBatchSizeConvDecoder\n",
    "\n",
    "# encoder = MLPEncoder(num_in=K, num_out=R, in_dim=split_data_shape)\n",
    "# decoder = MLPDecoder(num_in=N, num_out=K, in_dim=split_conv_output_shape)\n",
    "\n",
    "encoder = CatChannelConvEncoder(num_in=K, num_out=R, in_dim=split_data_shape)\n",
    "decoder = CatChannelConvDecoder(\n",
    "    num_in=N, num_out=K, in_dim=split_conv_output_data_shape\n",
    ")\n",
    "\n",
    "# encoder = CatBatchSizeConvEncoder(num_in=K, num_out=R, in_dim=split_data_shape)\n",
    "# decoder = CatBatchSizeConvDecoder(num_in=N, num_out=K, in_dim=split_conv_output_data_shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练 Encoder Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset: 60000\n",
      "image size:  torch.Size([1, 28, 28])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/4:  44%|████████▊           | 412/938 [00:58<01:14,  7.02it/s, loss=1.6] \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[43], line 57\u001b[0m\n\u001b[0;32m     54\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     56\u001b[0m images \u001b[38;5;241m=\u001b[39m images\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 57\u001b[0m ground_truth \u001b[38;5;241m=\u001b[39m \u001b[43mconv_segment\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimages\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     58\u001b[0m ground_truth \u001b[38;5;241m=\u001b[39m ground_truth\u001b[38;5;241m.\u001b[39mview(ground_truth\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m# forward\u001b[39;00m\n",
      "File \u001b[1;32md:\\Miniconda3\\envs\\ming\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Miniconda3\\envs\\ming\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\Miniconda3\\envs\\ming\\Lib\\site-packages\\torch\\nn\\modules\\container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32md:\\Miniconda3\\envs\\ming\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Miniconda3\\envs\\ming\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32me:\\Nonlinear-Erasure-Code\\src\\base_model\\MyModel1.py:24\u001b[0m, in \u001b[0;36mBasicBlock.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[0;32m     23\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv(x)\n\u001b[1;32m---> 24\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     25\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(out)\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n",
      "File \u001b[1;32md:\\Miniconda3\\envs\\ming\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Miniconda3\\envs\\ming\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32md:\\Miniconda3\\envs\\ming\\Lib\\site-packages\\torch\\nn\\modules\\batchnorm.py:175\u001b[0m, in \u001b[0;36m_BatchNorm.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    168\u001b[0m     bn_training \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_mean \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrunning_var \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m    170\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    171\u001b[0m \u001b[38;5;124;03mBuffers are only updated if they are to be tracked and we are in training mode. Thus they only need to be\u001b[39;00m\n\u001b[0;32m    172\u001b[0m \u001b[38;5;124;03mpassed when the update should occur (i.e. in training mode when they are tracked), or when buffer stats are\u001b[39;00m\n\u001b[0;32m    173\u001b[0m \u001b[38;5;124;03mused for normalization (i.e. in eval mode when buffers are not None).\u001b[39;00m\n\u001b[0;32m    174\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m--> 175\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    176\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    177\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# If buffers are not to be tracked, ensure that they won't be updated\u001b[39;49;00m\n\u001b[0;32m    178\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_mean\u001b[49m\n\u001b[0;32m    179\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\n\u001b[0;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    181\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrunning_var\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mnot\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrack_running_stats\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[0;32m    182\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    183\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    184\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbn_training\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    185\u001b[0m \u001b[43m    \u001b[49m\u001b[43mexponential_average_factor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    186\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    187\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32md:\\Miniconda3\\envs\\ming\\Lib\\site-packages\\torch\\nn\\functional.py:2482\u001b[0m, in \u001b[0;36mbatch_norm\u001b[1;34m(input, running_mean, running_var, weight, bias, training, momentum, eps)\u001b[0m\n\u001b[0;32m   2479\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m training:\n\u001b[0;32m   2480\u001b[0m     _verify_batch_size(\u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize())\n\u001b[1;32m-> 2482\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbatch_norm\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2483\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_mean\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrunning_var\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmomentum\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43meps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackends\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcudnn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menabled\u001b[49m\n\u001b[0;32m   2484\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "from dataset.image_dataset import ImageDataset\n",
    "from util.split_data import split_vector\n",
    "\n",
    "print(f\"Train dataset: {len(train_dataset)}\")\n",
    "print(\"image size: \", train_dataset[0][0].size())\n",
    "\n",
    "# 定义损失函数\n",
    "criterion = nn.MSELoss()\n",
    "criterion2 = nn.CrossEntropyLoss()\n",
    "optimizer_encoder = optim.SGD(encoder.parameters(), lr=0.001, momentum=0.9)\n",
    "optimizer_decoder = optim.SGD(decoder.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "conv_segment.to(device)\n",
    "fc_segment.to(device)\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n",
    "\n",
    "conv_segment.eval()\n",
    "fc_segment.eval()\n",
    "encoder.train()\n",
    "decoder.train()\n",
    "\n",
    "model.eval()\n",
    "for m in model.modules():\n",
    "    if isinstance(m, nn.BatchNorm2d):\n",
    "        m.track_running_stats = False\n",
    "\n",
    "loss_list = [[] for _ in range(epoch_num)]\n",
    "\n",
    "for epoch in range(epoch_num):\n",
    "    train_loader_tqdm = tqdm(\n",
    "        train_loader,\n",
    "        desc=f\"Epoch {epoch+1}/{epoch_num}\",\n",
    "        bar_format=\"{l_bar}{bar:20}{r_bar}\",\n",
    "    )\n",
    "    correct = 0\n",
    "    correct_truth = 0\n",
    "    total = 0\n",
    "    for images, labels in train_loader_tqdm:\n",
    "        # split image tensor(64, 3, 32, 32) -> [tensor(64, 3, 32, 8) * K]\n",
    "        images_list = []\n",
    "        for _1, _2, start, end in split_data_range:\n",
    "            images_list.append(images[:, :, :, start:end].clone())\n",
    "\n",
    "        images_list = [images.to(device) for images in images_list]\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        images = images.to(device)\n",
    "        ground_truth = conv_segment(images)\n",
    "        ground_truth = ground_truth.view(ground_truth.size(0), -1)\n",
    "\n",
    "        # forward\n",
    "        images_list += encoder(images_list)\n",
    "        output_list = []\n",
    "        for i in range(N):\n",
    "            output = conv_segment(images_list[i])\n",
    "            output_list.append(output)\n",
    "        # losed_output_list = lose_something(output_list, self.lose_device_index)\n",
    "        decoded_output_list = decoder(output_list)\n",
    "        output = torch.cat(decoded_output_list, dim=3)\n",
    "        output = output.view(output.size(0), -1)\n",
    "\n",
    "        loss = criterion(output, ground_truth)\n",
    "        # loss = criterion2(fc_segment(output), fc_segment(ground_truth))\n",
    "        loss_list[epoch].append(loss.item())\n",
    "\n",
    "        # backward\n",
    "        optimizer_encoder.zero_grad()\n",
    "        optimizer_decoder.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer_encoder.step()\n",
    "        optimizer_decoder.step()\n",
    "\n",
    "        # calculate accuracy\n",
    "        _, predicted = torch.max(fc_segment(output).data, 1)\n",
    "        _, predicted_truth = torch.max(fc_segment(ground_truth.data), 1)\n",
    "        # print(predicted)\n",
    "        # print(predicted_truth)\n",
    "        # print(labels)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        correct_truth += (predicted_truth == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "        train_loader_tqdm.set_postfix(loss=loss.item())\n",
    "\n",
    "    print(f\"Epoch: {epoch+1}, Loss: {loss.item()}\")\n",
    "    print(f\"Train Accuracy: {100 * correct / total}%\")\n",
    "    print(f\"Original Accuracy: {100 * correct_truth / total}%\")\n",
    "    # 27%\n",
    "    # 10%"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "保存模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "\n",
    "os.makedirs(os.path.dirname(encoder_path), exist_ok=True)\n",
    "os.makedirs(os.path.dirname(decoder_path), exist_ok=True)\n",
    "\n",
    "torch.save(encoder.state_dict(), encoder_path)\n",
    "torch.save(decoder.state_dict(), decoder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(*loss_list)\n",
    "\n",
    "path = save_dir + \"loss2.txt\"\n",
    "os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "with open(path, \"w\") as f:\n",
    "    for loss in loss_list:\n",
    "        f.write(\" \".join(map(str, loss)) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "with open(\"loss.txt\", \"r\") as f:\n",
    "    loss_list = [list(map(float, line.split())) for line in f.readlines()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "y = [e for l in loss_list for e in l]\n",
    "print(f\"记录的loss数量: {len(y)}\")\n",
    "print(f\"最后一个loss: {y[-1]}\")\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在测试集上评估 Encoder Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 加载模型\n",
    "encoder = MLPEncoder(num_in=K, num_out=R, in_dim=split_data_shape)\n",
    "decoder = MLPDecoder(num_in=N, num_out=K, in_dim=split_conv_output_shape)\n",
    "\n",
    "encoder.load_state_dict(torch.load(encoder_path))\n",
    "decoder.load_state_dict(torch.load(decoder_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "from dataset.image_dataset import ImageDataset\n",
    "from util.split_data import split_vector\n",
    "\n",
    "print(f\"Test dataset: {len(test_dataset)}\")\n",
    "print(\"image size: \", train_dataset[0][0].size())\n",
    "\n",
    "conv_segment.to(device)\n",
    "fc_segment.to(device)\n",
    "model.to(device)\n",
    "encoder.to(device)\n",
    "decoder.to(device)\n",
    "\n",
    "conv_segment.eval()\n",
    "fc_segment.eval()\n",
    "model.eval()\n",
    "encoder.eval()\n",
    "decoder.eval()\n",
    "\n",
    "original_correct = 0\n",
    "merge_correct = 0\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    test_loader_tqdm = tqdm(\n",
    "        test_loader,\n",
    "        desc=f\"Test\",\n",
    "        bar_format=\"{l_bar}{bar:20}{r_bar}\",\n",
    "    )\n",
    "    for images, labels in test_loader_tqdm:\n",
    "\n",
    "        # split image tensor(64, 3, 32, 32) -> [tensor(64, 3, 32, 8) * K]\n",
    "        images_list = []\n",
    "        for start, end in split_vector(\n",
    "            L=original_data_shape[2], k=K, l=split_data_shape[2]\n",
    "        ):\n",
    "            images_list.append(images[:, :, :, start:end].clone())\n",
    "\n",
    "        images_list = [images.to(device) for images in images_list]\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        images = images.to(device)\n",
    "        ground_truth = conv_segment(images)\n",
    "\n",
    "        _, predicted = torch.max(model(images).data, 1)\n",
    "        original_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        output = conv_segment(images)\n",
    "        output = output.view(output.size(0), -1)\n",
    "        output = fc_segment(output)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        merge_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        imageDataset_list = [\n",
    "            ImageDataset(images) for images in images_list + encoder(images_list)\n",
    "        ]\n",
    "        output_list = []\n",
    "        for i in range(N):\n",
    "            imageDataset = imageDataset_list[i]\n",
    "            output = conv_segment(imageDataset.images)\n",
    "            output_list.append(output)\n",
    "        # losed_output_list = lose_something(output_list, self.lose_device_index)\n",
    "        decoded_output_list = decoder(output_list)\n",
    "        output = torch.cat(decoded_output_list, dim=3)\n",
    "        output = output.view(output.size(0), -1)\n",
    "\n",
    "        _, predicted = torch.max(fc_segment(output).data, 1)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "print(f\"测试集总数: {total}\")\n",
    "print(\n",
    "    f\"原始模型 -> 预测正确数: {original_correct}, 预测准确率: {100 * original_correct / total}%\"\n",
    ")\n",
    "print(\n",
    "    f\"合并模型(conv+fc) -> 预测正确数: {merge_correct}, 预测准确率: {100 * merge_correct / total}%\"\n",
    ")\n",
    "print(\n",
    "    f\"使用Encoder和Decoder -> 预测正确数: {correct}, 预测准确率: {100 * correct / total}%\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练 base model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset: 60000\n",
      "Test dataset: 10000\n",
      "image size:  torch.Size([1, 28, 28])\n",
      "Device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/10: 100%|██████████| 938/938 [00:39<00:00, 24.00it/s, loss=0.0128] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10, Loss: 0.012757232412695885\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2/10: 100%|██████████| 938/938 [00:41<00:00, 22.78it/s, loss=0.0142] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/10, Loss: 0.014194248244166374\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 3/10: 100%|██████████| 938/938 [00:41<00:00, 22.42it/s, loss=0.0218] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/10, Loss: 0.021787161007523537\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 4/10: 100%|██████████| 938/938 [00:40<00:00, 23.21it/s, loss=0.00286]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/10, Loss: 0.002860612003132701\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 5/10: 100%|██████████| 938/938 [00:40<00:00, 23.15it/s, loss=0.0123]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/10, Loss: 0.012319878675043583\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 6/10: 100%|██████████| 938/938 [00:42<00:00, 21.84it/s, loss=0.0264]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/10, Loss: 0.02640596404671669\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 7/10: 100%|██████████| 938/938 [00:42<00:00, 22.22it/s, loss=0.00395] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/10, Loss: 0.003954804968088865\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 8/10: 100%|██████████| 938/938 [00:37<00:00, 25.24it/s, loss=0.00779] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/10, Loss: 0.007793494500219822\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 9/10: 100%|██████████| 938/938 [00:39<00:00, 23.48it/s, loss=0.00105] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/10, Loss: 0.0010497723706066608\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 10/10: 100%|██████████| 938/938 [00:41<00:00, 22.44it/s, loss=0.00354] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10/10, Loss: 0.003543086349964142\n",
      "训练集-> 总量: 60000, 正确数量: 59930, 准确率: 99.88333333333334%\n",
      "测试集-> 总量: 10000, 正确数量: 9917, 准确率: 99.17%\n",
      "训练时间: 0:06:47.128733\n",
      "训练集评估时间: 0:00:13.520933\n",
      "测试集评估时间: 0:00:02.320267\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from tqdm import tqdm\n",
    "import datetime\n",
    "import os\n",
    "\n",
    "\n",
    "print(f\"Train dataset: {len(train_dataset)}\")\n",
    "print(f\"Test dataset: {len(test_dataset)}\")\n",
    "print(\"image size: \", train_dataset[0][0].size())\n",
    "\n",
    "# 部署到设备\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Device: {device}\")\n",
    "model.to(device)\n",
    "\n",
    "# 定义损失函数和优化器\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0)\n",
    "\n",
    "t0 = datetime.datetime.now()\n",
    "# 训练循环\n",
    "model.train()  # 设置模型为训练模式\n",
    "num_epochs = 10  # 迭代次数\n",
    "for epoch in range(num_epochs):\n",
    "    # 使用 tqdm 包装训练数据加载器\n",
    "    train_loader_tqdm = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    for data, target in train_loader_tqdm:\n",
    "        # 将数据移动到设备上\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        # target 变为 one-hot 编码\n",
    "        target = (\n",
    "            torch.zeros(target.size(0), 10)\n",
    "            .to(device)\n",
    "            .scatter_(1, target.view(-1, 1), 1)\n",
    "        )\n",
    "\n",
    "        # 正向传播\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "\n",
    "        # 反向传播和优化\n",
    "        optimizer.zero_grad()  # 清除之前的梯度\n",
    "        loss.backward()  # 反向传播\n",
    "        optimizer.step()  # 更新权重\n",
    "\n",
    "        # 更新进度条的描述\n",
    "        train_loader_tqdm.set_postfix(loss=loss.item())\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Loss: {loss.item()}\")\n",
    "t1 = datetime.datetime.now()\n",
    "\n",
    "# 测试循环\n",
    "model.eval()  # 设置模型为评估模式\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():  # 在评估过程中不计算梯度\n",
    "    for data, target in train_loader:\n",
    "        # 将数据移动到设备上\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        output = model(data)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        total += target.size(0)\n",
    "        correct += (predicted == target).sum().item()\n",
    "\n",
    "print(f\"训练集-> 总量: {total}, 正确数量: {correct}, 准确率: {100 * correct / total}%\")\n",
    "t2 = datetime.datetime.now()\n",
    "\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():  # 在评估过程中不计算梯度\n",
    "    for data, target in test_loader:\n",
    "        # 将数据移动到设备上\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        output = model(data)\n",
    "        _, predicted = torch.max(output.data, 1)\n",
    "        total += target.size(0)\n",
    "        correct += (predicted == target).sum().item()\n",
    "\n",
    "print(f\"测试集-> 总量: {total}, 正确数量: {correct}, 准确率: {100 * correct / total}%\")\n",
    "t3 = datetime.datetime.now()\n",
    "\n",
    "print(f\"训练时间: {t1 - t0}\")\n",
    "print(f\"训练集评估时间: {t2 - t1}\")\n",
    "print(f\"测试集评估时间: {t3 - t2}\")\n",
    "\n",
    "# 保存模型\n",
    "now = datetime.datetime.now()\n",
    "date = now.strftime(\"%Y_%m_%d\")\n",
    "filepath = f\"base_model/{TASK_CONFIG['MODEL']}/{TASK_CONFIG['TASK']}/{TASK_CONFIG['DATE']}/model.pth\"\n",
    "dirpath = os.path.dirname(filepath)\n",
    "if not os.path.exists(dirpath):\n",
    "    os.makedirs(dirpath)\n",
    "torch.save(model.state_dict(), filepath)\n",
    "\n",
    "# 读取模型\n",
    "# model = LeNet5(input_dim=(1, 28, 28), num_classes=10)\n",
    "# model.load_state_dict(torch.load(filepath))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ming",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
